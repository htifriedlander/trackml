{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hits: tensor of samples x timesteps x features\n",
    "    bounds: K:V of x_max, x_min, y_max, y_min, z_max, z_min\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "assuming input is samples x [x_val,y_val,z_val]\n",
    "\"\"\"\n",
    "def calc_norm(xyz, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    norm_x = ((xyz[0] - x_min)/(x_max - x_min))\n",
    "    norm_y = ((xyz[1] - y_min)/(y_max - y_min))\n",
    "    norm_z = ((xyz[2] - z_min)/(z_max - z_min))\n",
    "    return [norm_x,norm_y,norm_z]\n",
    "\n",
    "def norm_hits(hits, bounds):\n",
    "    assert ('x_max' and 'x_min' and 'y_max' and 'y_min' and 'z_max' and 'z_min' in bounds) or ('r_max' and 'r_min' and 'phi_max' and 'phi_min' and 'z_max' and 'z_min' in bounds)\n",
    "    return np.apply_along_axis(calc_norm, 1, hits, x_min = bounds['x_min'], x_max = bounds['x_max'], y_min = bounds['y_min'], y_max = bounds['y_max'], z_min = bounds['z_min'], z_max = bounds['z_max']) \n",
    "\n",
    "\n",
    "def calc_bounds():\n",
    "    bounds={'x_max':0, 'x_min':0, 'y_max':0, 'y_min':0, 'z_max':0, 'z_min':0}\n",
    "    for num in range(1000,1099):\n",
    "        file_name = 'event00000' + str(num)\n",
    "        hits, cells, particles, truth = load_event('data/train_sample/'+file_name)\n",
    "        event_x_max, event_x_min, event_y_max, event_y_min, event_z_max, event_z_min = hits['x'].max(), hits['x'].min(), hits['y'].max(), hits['y'].min(), hits['z'].max(), hits['z'].min() \n",
    "        if bounds['x_max'] < event_x_max:\n",
    "            bounds['x_max'] = event_x_max\n",
    "        if bounds['x_min'] > event_x_min:\n",
    "            bounds['x_min'] = event_x_min\n",
    "        if bounds['y_max'] < event_y_max:\n",
    "            bounds['y_max'] = event_y_max\n",
    "        if bounds['y_min'] > event_y_min:\n",
    "            bounds['y_min'] = event_y_min\n",
    "        if bounds['z_max'] < event_z_max:\n",
    "            bounds['z_max'] = event_z_max\n",
    "        if bounds['z_min'] > event_y_min:\n",
    "            bounds['z_min'] = event_y_min\n",
    "    return bounds\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "input: hits and truth dfs\n",
    "\n",
    "output: df of particle_id as index and list of particles \n",
    "\"\"\"\n",
    "\n",
    "def gen_tracks(truth_df):\n",
    "    assert(isinstance(truth_df, pd.DataFrame))\n",
    "    #print(truth_df)\n",
    "    truth_df['dist'] = np.sqrt(truth_df['tx']**2+truth_df['ty']**2+truth_df['tz']**2)\n",
    "    grouped = truth_df.groupby('particle_id')['hit_id','dist']\n",
    "    a = grouped.apply(lambda x: x.sort_values(by=['dist'],ascending=True))\n",
    "    final = a.groupby('particle_id')['hit_id'].apply(np.array)\n",
    "    return final \n",
    "\n",
    "##there's still a problem with how to deal with hits that have a particle id of 0\n",
    "\n",
    "def batch_iter(truth_df, batch_size):\n",
    "    tracks = gen_tracks(truth_df).values\n",
    "    np.random.shuffle(tracks) \n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "        \n",
    "def get_data(max_seq_len, batch_size, feature_len, truth_df, hits_df):\n",
    "    hits = hits_df\n",
    "    max_seq_len = max_seq_len\n",
    "    b_size = batch_size\n",
    "    features = feature_len #xyz or phi r z\n",
    "    all_data = list(batch_iter(truth_df,b_size))\n",
    "    \n",
    "    #print(all_data)\n",
    "    for result in all_data:\n",
    "        batch = []\n",
    "        labels_tensor = []\n",
    "        for track_list in result:\n",
    "            for hit_id in track_list:\n",
    "                hit_coord = []\n",
    "                track = []\n",
    "                for elem in hit_id:\n",
    "                    x,y,z = hits.loc[hits['hit_id']== elem]['x'].item(), hits.loc[hits['hit_id']== elem]['y'].item(), hits.loc[hits['hit_id']== elem]['z'].item()\n",
    "                    r,phi,z = cartesian_to_3d_polar(x,y,z)\n",
    "                    hit_coord = [r,phi,z]\n",
    "                    track.append(hit_coord)\n",
    "                zeros_to_add = max_seq_len - len(track)\n",
    "                if zeros_to_add > 0:\n",
    "                    add_array = np.zeros((zeros_to_add,3))\n",
    "                    np_data = np.array(track) \n",
    "                    padded_track_data  = np.append(np_data,add_array,axis=0)\n",
    "                elif zeros_to_add < 0:\n",
    "                    modded_track = track[:zeros_to_add]\n",
    "                    padded_track_data = np.array(modded_track)\n",
    "                else:\n",
    "                    padded_track_data = np.array(track)\n",
    "            row_label = padded_track_data[1:]\n",
    "            padded_row_label = np.append(row_label, np.zeros((1,3)),axis=0)\n",
    "            labels_tensor.append(padded_row_label)\n",
    "            batch.append(padded_track_data)\n",
    "        padded_batch_data = np.array(batch)\n",
    "        padded_labels = np.array(labels_tensor)\n",
    "        #print(padded_labels)\n",
    "        yield padded_batch_data, padded_labels\n",
    "        \n",
    "def next_batch(max_seq_len, batch_size, feature_len):\n",
    "    all_data = load_dataset('data/train_sample/', parts=['hits','truth'])\n",
    "    for data in all_data:\n",
    "        hit_df, truth_df = data[1], data[2]\n",
    "        yield from get_data(max_seq_len, batch_size, feature_len, truth_df, hit_df)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 7\n",
    "n_input_features = 3\n",
    "n_neurons = 200\n",
    "n_outputs = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input' )\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs], name='label')\n",
    "\n",
    "#uses tanh for inner activations\n",
    "lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "#lstm = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units = n_neurons)\n",
    "\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "print_op = tf.Print(rnn_outputs,[rnn_outputs], name='print_pred')\n",
    "loss_diff = tf.reduce_sum(tf.square(rnn_outputs-y), 2)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(y),2))\n",
    "mask_loss = mask*loss_diff\n",
    "batch_loss = tf.reduce_mean(mask_loss)\n",
    "\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = opt.minimize(batch_loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "data_itr = next_batch(n_steps, batch_size,n_input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints-0\n",
      "0 \tMSE: 11780.666\n",
      "100 \tMSE: 53274.473\n",
      "200 \tMSE: 14221.977\n",
      "300 \tMSE: 12399.667\n",
      "400 \tMSE: 6165.5723\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 500\n",
    "batch_size = 50 \n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', batch_loss)\n",
    "        tf.summary.histogram('histogram_loss', batch_loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    #saver.restore(sess, \"./lstm_weight1sckpt\")\n",
    "    init.run()\n",
    "    initial_step = global_step.eval()\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        x_batch, y_batch = next(data_itr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        _, loss_batch, summary, test_print = sess.run([train_op, batch_loss, summary_op, print_op], feed_dict={X:x_batch, y:y_batch})\n",
    "        writer.add_summary(summary, global_step=iteration)\n",
    "        \n",
    "        if iteration %100 ==0:\n",
    "            mse = batch_loss.eval(feed_dict={X:x_batch, y:y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "            saver.save(sess, './checkpoints', iteration)\n",
    "    #saver.save(sess,\"./lstm_weights1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
