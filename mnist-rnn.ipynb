{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "#import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 20\n",
    "display_step = 200\n",
    "\n",
    "num_inputs = 28 #because of mnist\n",
    "timesteps = 28 #timesteps for each sample\n",
    "num_hidden = 256 #hidden layer nodes\n",
    "num_classes = 10 #10 digits\n",
    "\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32, shape=(None, timesteps, num_inputs))\n",
    "y = tf.placeholder(tf.float32, shape=(None, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining weights\n",
    "w = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "#defining bias\n",
    "b = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-cdd825d12fe7>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = tf.unstack(X, timesteps, 1) #splitting the tensor x into timesteps tensors\n",
    "\n",
    "simple_lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "outputs, states = rnn.static_rnn(simple_lstm_cell, input, dtype='float32')\n",
    "pred = tf.matmul(outputs[-1], w)+b\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0\n",
      "Acc:  0.125\n",
      "loss:  2.6158276\n",
      "_________\n",
      "iter:  10\n",
      "Acc:  0.40625\n",
      "loss:  1.6605942\n",
      "_________\n",
      "iter:  20\n",
      "Acc:  0.5546875\n",
      "loss:  1.2751505\n",
      "_________\n",
      "iter:  30\n",
      "Acc:  0.65625\n",
      "loss:  1.025379\n",
      "_________\n",
      "iter:  40\n",
      "Acc:  0.71875\n",
      "loss:  0.88657534\n",
      "_________\n",
      "iter:  50\n",
      "Acc:  0.828125\n",
      "loss:  0.67418873\n",
      "_________\n",
      "iter:  60\n",
      "Acc:  0.796875\n",
      "loss:  0.6796967\n",
      "_________\n",
      "iter:  70\n",
      "Acc:  0.8046875\n",
      "loss:  0.59254766\n",
      "_________\n",
      "iter:  80\n",
      "Acc:  0.875\n",
      "loss:  0.39182937\n",
      "_________\n",
      "iter:  90\n",
      "Acc:  0.78125\n",
      "loss:  0.66320306\n",
      "_________\n",
      "iter:  100\n",
      "Acc:  0.875\n",
      "loss:  0.37154\n",
      "_________\n",
      "iter:  110\n",
      "Acc:  0.890625\n",
      "loss:  0.30587476\n",
      "_________\n",
      "iter:  120\n",
      "Acc:  0.9140625\n",
      "loss:  0.22738583\n",
      "_________\n",
      "iter:  130\n",
      "Acc:  0.8515625\n",
      "loss:  0.40274885\n",
      "_________\n",
      "iter:  140\n",
      "Acc:  0.921875\n",
      "loss:  0.19469392\n",
      "_________\n",
      "iter:  150\n",
      "Acc:  0.875\n",
      "loss:  0.30552438\n",
      "_________\n",
      "iter:  160\n",
      "Acc:  0.9296875\n",
      "loss:  0.20958424\n",
      "_________\n",
      "iter:  170\n",
      "Acc:  0.9140625\n",
      "loss:  0.25573808\n",
      "_________\n",
      "iter:  180\n",
      "Acc:  0.90625\n",
      "loss:  0.2610083\n",
      "_________\n",
      "iter:  190\n",
      "Acc:  0.921875\n",
      "loss:  0.26926938\n",
      "_________\n",
      "iter:  200\n",
      "Acc:  0.90625\n",
      "loss:  0.23536634\n",
      "_________\n",
      "iter:  210\n",
      "Acc:  0.9296875\n",
      "loss:  0.25112364\n",
      "_________\n",
      "iter:  220\n",
      "Acc:  0.9453125\n",
      "loss:  0.170539\n",
      "_________\n",
      "iter:  230\n",
      "Acc:  0.9453125\n",
      "loss:  0.22161397\n",
      "_________\n",
      "iter:  240\n",
      "Acc:  0.9609375\n",
      "loss:  0.11840491\n",
      "_________\n",
      "iter:  250\n",
      "Acc:  0.9375\n",
      "loss:  0.19629009\n",
      "_________\n",
      "iter:  260\n",
      "Acc:  0.96875\n",
      "loss:  0.11220642\n",
      "_________\n",
      "iter:  270\n",
      "Acc:  0.921875\n",
      "loss:  0.18391241\n",
      "_________\n",
      "iter:  280\n",
      "Acc:  0.9375\n",
      "loss:  0.22046176\n",
      "_________\n",
      "iter:  290\n",
      "Acc:  0.921875\n",
      "loss:  0.23670419\n",
      "_________\n",
      "iter:  300\n",
      "Acc:  0.9453125\n",
      "loss:  0.15035215\n",
      "_________\n",
      "iter:  310\n",
      "Acc:  0.9296875\n",
      "loss:  0.23495877\n",
      "_________\n",
      "iter:  320\n",
      "Acc:  0.9375\n",
      "loss:  0.20941465\n",
      "_________\n",
      "iter:  330\n",
      "Acc:  0.9765625\n",
      "loss:  0.0979007\n",
      "_________\n",
      "iter:  340\n",
      "Acc:  0.9375\n",
      "loss:  0.13302515\n",
      "_________\n",
      "iter:  350\n",
      "Acc:  0.9609375\n",
      "loss:  0.18306543\n",
      "_________\n",
      "iter:  360\n",
      "Acc:  0.8984375\n",
      "loss:  0.37409493\n",
      "_________\n",
      "iter:  370\n",
      "Acc:  0.9140625\n",
      "loss:  0.25400558\n",
      "_________\n",
      "iter:  380\n",
      "Acc:  0.9140625\n",
      "loss:  0.16915801\n",
      "_________\n",
      "iter:  390\n",
      "Acc:  0.953125\n",
      "loss:  0.14425106\n",
      "_________\n",
      "iter:  400\n",
      "Acc:  0.9765625\n",
      "loss:  0.06367349\n",
      "_________\n",
      "iter:  410\n",
      "Acc:  0.96875\n",
      "loss:  0.07610875\n",
      "_________\n",
      "iter:  420\n",
      "Acc:  0.9375\n",
      "loss:  0.14587551\n",
      "_________\n",
      "iter:  430\n",
      "Acc:  0.9609375\n",
      "loss:  0.10411452\n",
      "_________\n",
      "iter:  440\n",
      "Acc:  0.953125\n",
      "loss:  0.11479239\n",
      "_________\n",
      "iter:  450\n",
      "Acc:  0.9609375\n",
      "loss:  0.11749442\n",
      "_________\n",
      "iter:  460\n",
      "Acc:  0.96875\n",
      "loss:  0.13948223\n",
      "_________\n",
      "iter:  470\n",
      "Acc:  0.9609375\n",
      "loss:  0.14026588\n",
      "_________\n",
      "iter:  480\n",
      "Acc:  0.96875\n",
      "loss:  0.08450224\n",
      "_________\n",
      "iter:  490\n",
      "Acc:  0.96875\n",
      "loss:  0.12105672\n",
      "_________\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(500):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size = batch_size)\n",
    "        #print(batch_x.shape)\n",
    "        reshaped_x = batch_x.reshape((batch_size,timesteps,num_inputs))\n",
    "        #print(reshaped_x.shape)\n",
    "        sess.run(opt, feed_dict={X:reshaped_x, y:batch_y})\n",
    "        \n",
    "        if i %10==0:\n",
    "            accuracy = sess.run(acc, feed_dict={X:reshaped_x, y: batch_y})\n",
    "            loss_num = sess.run(loss, feed_dict={X:reshaped_x, y: batch_y})\n",
    "            print(\"iter: \", str(i))\n",
    "            print(\"Acc: \",str(accuracy))\n",
    "            print(\"loss: \",str(loss_num))\n",
    "            print(\"_________\")\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
