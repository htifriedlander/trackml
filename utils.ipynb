{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions necessary for pre-processing\n",
    "\n",
    "\n",
    "\n",
    "use get_mappings() to see all unique {particles:superstrips} mappings for all training samples (first 100 events of total training data)\n",
    "<br>\n",
    "\n",
    "use calc_mappings() to generate all unqiue {particles:superstrips} mappings for all training samples + writes them to file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "import timeit\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import os\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_single_train_event(event_number):\n",
    "    train_dir = 'data/train_sample/'\n",
    "    hits, cells, particles, truth = load_event(train_dir+'event00000'+str(event_number))\n",
    "    return hits, cells, particles, truth\n",
    "\n",
    "\n",
    "def process_single_event(event_number):\n",
    "    start = time.time()\n",
    "    file_name = 'event00000' + str(event_number)\n",
    "    event_id = file_name\n",
    "    hits_orig, cells, particles, truth = load_event('data/train_sample/'+event_id)\n",
    "    merge_by_hit_ids = hits_orig.merge(truth, how = 'inner', on = 'hit_id')\n",
    "    merge_by_particle_ids = merge_by_hit_ids.merge(particles, how = 'inner', on = 'particle_id')\n",
    "    partid_dict = {}\n",
    "    hitloc_dict = {}\n",
    "    for row in merge_by_particle_ids.itertuples():\n",
    "        particleID = row.__getattribute__('particle_id')\n",
    "        volID = row.__getattribute__('volume_id')\n",
    "        layerID = row.__getattribute__('layer_id') \n",
    "        modID = row.__getattribute__('module_id')\n",
    "        hitID = row.__getattribute__('hit_id')\n",
    "        key_name = event_id + '-' + str(particleID)\n",
    "        hitloc_dict = {'hit_id':hitID, 'volume_id':volID, 'layer_id':layerID, 'module_id': modID}\n",
    "        if key_name in partid_dict:\n",
    "            partid_dict[key_name].append(hitloc_dict)\n",
    "        else:\n",
    "            partid_dict[key_name] = [hitloc_dict]\n",
    "    \n",
    "    with open('mappings/'+event_id + '.json','w') as outfile:\n",
    "        json.dump(partid_dict, outfile)\n",
    "    \n",
    "    end = time.time()\n",
    "    return partid_dict\n",
    "\n",
    "def calc_mappings():\n",
    "    start = time.time()\n",
    "    p = Pool(None) ##uses all available cpu cores, 32 in this case\n",
    "    p.map(process_single_event, list(range(1000,1100))) #just contents of training-sample for now, i.e first 100 events \n",
    "    data_dir = 'mappings'\n",
    "    all_data = {}\n",
    "    for folder, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(folder,file)) as file:\n",
    "                    single_event_info = json.load(file)\n",
    "                    all_data.update(single_event_info)\n",
    "    with open('training-sample-mappings.json','w') as outfile:\n",
    "        json.dump(all_data, outfile)\n",
    "    end = time.time()\n",
    "    print('elasped time:'+str(end-start))\n",
    "    return\n",
    "    \n",
    "def get_mappings():\n",
    "    path = 'training-sample-mappings.json'\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_test_events ():\n",
    "    itr = load_dataset ('./data/test_data', parts = ['hits'])\n",
    "    for event_data in itr:\n",
    "        event_id = event_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " #print(timeit.timeit(\"calc_mappings()\",number=1, setup=\"from __main__ import calc_mappings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can prob be further optimzed \n",
    "#dont user toFile param for now\n",
    "def find_top_hyperstrips(k,toFile=False):\n",
    "    \"\"\"\n",
    "        k: num of hyperstrips we're considering \n",
    "        toFile: flag for if we want to write the returned {K:V} to file\n",
    "        \n",
    "        \n",
    "        returns a {(vol_id,layer_id,module_id):(cx,cy,cz)} of superstrips from the first k-most hyperstrips that contain the most tracks   \n",
    "    \"\"\"\n",
    "    \n",
    "    data = get_mappings()\n",
    "    hyperstrips_dict = {}\n",
    "    for key, lists in data.items():\n",
    "        superstrip_info = []\n",
    "        hyperstrip_info = [] #hyperstrip is a subset of superstrips for a given track\n",
    "        for small_dictionaries in lists:\n",
    "            if 'hit_id' in small_dictionaries:\n",
    "                del small_dictionaries['hit_id']\n",
    "            superstrip = (small_dictionaries['volume_id'], small_dictionaries['layer_id'], small_dictionaries['module_id'])\n",
    "            hyperstrip_info.append(superstrip)\n",
    "\n",
    "        hyperstrip = tuple(hyperstrip_info)\n",
    "        #print(hyperstrip)\n",
    "        if hyperstrip in hyperstrips_dict:\n",
    "            #print('found one!')\n",
    "            hyperstrips_dict[hyperstrip] += 1\n",
    "        else:\n",
    "            hyperstrips_dict[hyperstrip] = 1\n",
    "\n",
    "    sorted_hyperstrips = sorted(hyperstrips_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    for idx, items in enumerate(sorted_hyperstrips):\n",
    "        hyperstrip, count = items\n",
    "        if len(hyperstrip) > 1 :\n",
    "            start = idx\n",
    "            #print(sorted_hyperstrips[start: start + 500])\n",
    "            someslice = sorted_hyperstrips[start:start+k]\n",
    "            break\n",
    "\n",
    "    #creates a dicitonary for frequency of hits in hyperstrips\n",
    "    lengths_freq = {}\n",
    "    for elem in someslice:\n",
    "        hyperstrip, number_of_tracks = elem \n",
    "        hyperstrip_len = len(hyperstrip)\n",
    "        if hyperstrip_len in lengths_freq:\n",
    "            lengths_freq[hyperstrip_len] += 1\n",
    "        else:\n",
    "            lengths_freq[hyperstrip_len] = 1\n",
    "\n",
    "    detector_info_path = 'data/detectors.csv'\n",
    "    detectors = pd.read_csv(detector_info_path)\n",
    "\n",
    "    superstrip_locs = {}\n",
    "    json_data = {}\n",
    "    for hyperstrip in someslice:\n",
    "        subset_of_strips, tracks = hyperstrip\n",
    "        for superstrip in subset_of_strips:\n",
    "            vol, lay, mod = superstrip\n",
    "            x = detectors.loc[(detectors['volume_id'] == vol) & (detectors['layer_id'] == lay) & (detectors['module_id'] == mod)]['cx'].item()\n",
    "            y = detectors.loc[(detectors['volume_id'] == vol) & (detectors['layer_id'] == lay) & (detectors['module_id'] == mod)]['cy'].item()\n",
    "            z = detectors.loc[(detectors['volume_id'] == vol) & (detectors['layer_id'] == lay) & (detectors['module_id'] == mod)]['cz'].item()\n",
    "            superstrip_locs[superstrip] = (x,y,z) \n",
    "            json_data[str(superstrip)] = (x,y,z)\n",
    "    if toFile:\n",
    "        with open('top-hyperstrips.json','w') as outfile:\n",
    "            json.dump(json_data, outfile)\n",
    "    return superstrip_locs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Xs, Ys, Zs = hits_df['x'].values, hits_df['y'].values, hits_df['z'].values\\n    #Rs, Phis, _ = cartesian_to_3d_polar(Xs, Ys, Zs)\\n    #hits_df['r'], hits_df['phi'] = Rs, Phis\\n    \\n    \\n    #test_ref_r = hits_df.loc[(hits_df['hit_id'] == ref_hit_id)]['r']\\n    #print(type(test_ref_r))\\n    \\n    \\n    #print('ref_r shape: '+str(ref_r.shape))\\n    dist = np.sqrt((Xs - x)**2 + (Ys - y)**2 + (Zs - z)**2)\\n    hits_df['dist'] = dist\\n    current_min = hits_df.iloc[0].dist\\n    best_id = hits_df.iloc[0].hit_id\\n    for row in hits_df.itertuples():\\n        dist = row.__getattribute__('dist')\\n        #print(dist)\\n        hit_id = row.__getattribute__('hit_id')\\n        if current_min > dist:\\n            current_min = dist\\n            best_id = hit_id\\n        \\n    return best_id\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def closestHit(x, y, z, hits):\n",
    "    hitList = hits.as_matrix(columns=hits.columns[0:4])\n",
    "    minDist = np.sqrt((x - hitList[0][1])**2 + (y - hitList[0][2])**2 + (z - hitList[0][3])**2)\n",
    "    for index, hit in enumerate(hitList):\n",
    "        dist = np.sqrt((x - hit[1])**2 + (y - hit[2])**2 + (z - hit[3])**2)\n",
    "        if dist < minDist:\n",
    "            minDist = dist\n",
    "            minDistHitID = hit[0]\n",
    "    return minDistHitID\n",
    "\n",
    "\n",
    "\n",
    "def calc_dist(x,y,z, hits_array):\n",
    "    min_dist = np.Inf\n",
    "    best_id = 1\n",
    "    dist_squared = (hits_array[:,1]-x)**2 + (hits_array[:,2]-y)**2 + (hits_array[:,3]-z)**2\n",
    "    index = np.argmin (dist_squared)\n",
    "    return (hits_array[index,0], dist_squared[index])\n",
    "    for hit in hits_array:\n",
    "        dist_squared = (hit[1]-x)**2 + (hit[2]-y)**2 + (hit[3]-z)**2\n",
    "        if (dist_squared < min_dist):\n",
    "            min_dist = dist_squared\n",
    "            best_id = hit[0]\n",
    "    return best_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_3d_polar(x, y, z):\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    #s  = np.sin(phi)\n",
    "    #c  = np.cos(phi)\n",
    "    return r, phi, z\n",
    "\n",
    "def polar_to_cartesian(r, phi, z):\n",
    "    x = r*np.cos(phi)\n",
    "    y = r*np.sin(phi)\n",
    "    return [x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hits: tensor of samples x timesteps x features\n",
    "    bounds: K:V of x_max, x_min, y_max, y_min, z_max, z_min\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "assuming input is samples x [x_val,y_val,z_val]\n",
    "\"\"\"\n",
    "def calc_norm(xyz, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    norm_x = ((xyz[0] - x_min)/(x_max - x_min))\n",
    "    norm_y = ((xyz[1] - y_min)/(y_max - y_min))\n",
    "    norm_z = ((xyz[2] - z_min)/(z_max - z_min))\n",
    "    return [norm_x,norm_y,norm_z]\n",
    "\n",
    "def norm_hits(hits, bounds):\n",
    "    assert ('x_max' and 'x_min' and 'y_max' and 'y_min' and 'z_max' and 'z_min' in bounds) or ('r_max' and 'r_min' and 'phi_max' and 'phi_min' and 'z_max' and 'z_min' in bounds)\n",
    "    return np.apply_along_axis(calc_norm, 1, hits, x_min = bounds['x_min'], x_max = bounds['x_max'], y_min = bounds['y_min'], y_max = bounds['y_max'], z_min = bounds['z_min'], z_max = bounds['z_max']) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "input: hits and truth dfs\n",
    "\n",
    "output: df of particle_id as index and list of particles \n",
    "\"\"\"\n",
    "\n",
    "def gen_tracks(truth_df):\n",
    "    assert(isinstance(truth_df, pd.DataFrame))\n",
    "    #print(truth_df)\n",
    "    truth_df['dist'] = np.sqrt(truth_df['tx']**2+truth_df['ty']**2+truth_df['tz']**2)\n",
    "    \n",
    "    grouped = truth_df.groupby('particle_id')['hit_id','dist']\n",
    "    \n",
    "    a = grouped.apply(lambda x: x.sort_values( by=['dist'],ascending=True))\n",
    "    final = a.groupby('particle_id')['hit_id'].apply(np.array)\n",
    "\n",
    "    return final \n",
    "\n",
    "##there's still a problem with how to deal with hits that have a particle id of 0\n",
    "\n",
    "def batch_iter(truth_df, batch_size):\n",
    "    tracks = gen_tracks(truth_df).values\n",
    "    np.random.shuffle(tracks) \n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "\n",
    "def get_data(max_seq_len, batch_size, feature_len, truth_df, hits_df, simple = False, full_tracks = False, ideal = False):\n",
    "    hits = hits_df\n",
    "    max_seq_len = max_seq_len\n",
    "    if ideal is True:\n",
    "        full_tracks = True\n",
    "        simple = True\n",
    "    if full_tracks is True:\n",
    "        max_seq_len = 10\n",
    "        simple = True\n",
    "    b_size = batch_size\n",
    "    features = feature_len #xyz or phi r z\n",
    "    if simple is True:\n",
    "        all_data = list(simple_batch_iter(hits_df, truth_df, batch_size, full_tracks, ideal))\n",
    "    else:\n",
    "        all_data = list(batch_iter(truth_df,b_size))\n",
    "    \n",
    "    #print(all_data)\n",
    "    for result in all_data:\n",
    "        batch = []\n",
    "        batch_lv = []\n",
    "        labels_tensor = []\n",
    "        labels_tensor_lv = []\n",
    "        for track_list in result:\n",
    "            for hit_id in track_list:\n",
    "                hit_coord = []\n",
    "                track = []\n",
    "                track_lb = []\n",
    "                lv_pair = []\n",
    "                lv_tensor = []\n",
    "                label_coord = []\n",
    "                for elem in hit_id:\n",
    "                    x, y, z, layer_id, volume_id = hits.loc[hits['hit_id']== elem]['x'].item(), hits.loc[hits['hit_id']== elem]['y'].item(), hits.loc[hits['hit_id']== elem]['z'].item(), hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    r,phi,z = cartesian_to_3d_polar(x,y,z)\n",
    "                    hit_coord = [r,phi,z,layer_id, volume_id]\n",
    "                    label_coord = [r,phi,z]\n",
    "                    track.append(hit_coord)\n",
    "                    track_lb.append(label_coord)\n",
    "                    layer, volume = hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    lv_pair = [layer, volume]\n",
    "                    lv_tensor.append(lv_pair)\n",
    "                zeros_to_add = max_seq_len - len(track)\n",
    "                if zeros_to_add > 0:\n",
    "                    add_array = np.zeros((zeros_to_add,feature_len))\n",
    "                    add_array_lb = np.zeros((zeros_to_add, 3)) #3 is hardcoded for xyz/rphiz\n",
    "                    add_array_lv = np.zeros((zeros_to_add, 2))\n",
    "                    np_data = np.array(track)\n",
    "                    np_data_lb = np.array(track_lb)\n",
    "                    np_data_lv = np.array(lv_tensor)\n",
    "                    padded_track_data  = np.append(np_data,add_array,axis=0)\n",
    "                \n",
    "                    padded_track_data_lb = np.append(np_data_lb, add_array_lb, axis=0)\n",
    "                    padded_track_data_lv = np.append(np_data_lv, add_array_lv, axis=0)\n",
    "                elif zeros_to_add < 0:\n",
    "                    modded_track = track[:zeros_to_add]\n",
    "                    modded_track_lv = lv_tensor[:zeros_to_add]\n",
    "                    modded_track_lb = track_lb[:zeros_to_add]\n",
    "                    padded_track_data_lb = np.array(modded_track_lb)\n",
    "                    padded_track_data = np.array(modded_track)\n",
    "                    padded_track_data_lv = np.array(modded_track_lv)\n",
    "                else:\n",
    "                    padded_track_data_lb = np.array(track_lb)\n",
    "                    padded_track_data = np.array(track)\n",
    "                    padded_track_data_lv = np.array(lv_tensor)\n",
    "            \n",
    "            row_label = padded_track_data_lb[1:]\n",
    "            row_label_lv = padded_track_data_lv[1:]\n",
    "            padded_row_label = np.append(row_label, np.zeros((1,3)), axis=0) #hardcoded 3 for xyz/rphiz\n",
    "            padded_row_label_lv = np.append(row_label_lv, np.zeros((1,2)), axis=0)\n",
    "            labels_tensor.append(padded_row_label)\n",
    "            labels_tensor_lv.append(padded_row_label_lv)\n",
    "            batch.append(padded_track_data)\n",
    "            batch_lv.append(padded_track_data_lv)\n",
    "            \n",
    "        padded_batch_data = np.array(batch)\n",
    "        padded_batch_data_lv = np.array(batch_lv)\n",
    "        padded_labels = np.array(labels_tensor)\n",
    "        padded_labels_lv = np.array(labels_tensor_lv)\n",
    "        #print(padded_labels)\n",
    "        #print(padded_labels_lv)\n",
    "        yield padded_batch_data, padded_labels, padded_batch_data_lv, padded_labels_lv\n",
    "        \n",
    "def next_batch(max_seq_len, batch_size, feature_len, simple = False, full_tracks = False, ideal = False, full_data = False):\n",
    "    if full_data:\n",
    "        path = '/mnt/hdd/trackml_full_data/all_train/'\n",
    "        all_data = load_dataset(path,parts=['hits','truth'])\n",
    "    else:    \n",
    "        all_data = load_dataset('data/train_sample/', parts=['hits','truth'])\n",
    "    for data in all_data:\n",
    "        hit_df, truth_df = data[1], data[2]\n",
    "        yield from get_data(max_seq_len, batch_size, feature_len, truth_df, hit_df, simple, full_tracks, ideal)\n",
    "        \n",
    "def get_hit_info(x, y, z): #returns layer, volume, and hit_id\n",
    "    row = hits.loc[(hits['x'] == x) & (hits['y'] == y) & (hits['z'] == z)]\n",
    "    return (int(row.iloc[0][4]), int(row.iloc[0][5]), int(row.iloc[0][0]))\n",
    "\n",
    "def next_seed(hits_from_seeds, batch_size=1):\n",
    "    for seed in hits_from_seeds:\n",
    "        track_seed = seed.reshape(1,3,5)\n",
    "        yield track_seed.tolist()\n",
    "        \n",
    "def distance_between_two_points(p1, p2):\n",
    "    distance = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2 + (p2[2] - p1[2])**2)\n",
    "    return distance\n",
    "\n",
    "def get_xyz(hit_id):\n",
    "    xyz = hits[hits[\"hit_id\"] == hit_id].iloc[:,1:4].values.tolist()[0]\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cylinder hit related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_cylinder (volume_id, layer_id):\n",
    "    return (((volume_id == 8 or volume_id == 13) and layer_id in [2,4,6,8])\n",
    "           or (volume_id == 17 and layer_id in [2,4]))\n",
    "\n",
    "\n",
    "def layer_number (volume_id, layer_id):\n",
    "    assert (volume_id in [8, 13, 17]), \"This volume_id is not in a cylinder!\"\n",
    "    assert (((volume_id == 8 or volume_id == 13) and layer_id in [2,4,6,8])\n",
    "           or (volume_id == 17 and layer_id in [2,4])), \"This layer_id is not in this volume_id\"\n",
    "    diction = {(8,2):1, (8,4):2, (8,6):3, (8,8):4, (13,2):5, (13,4):6, (13,6):7, (13,8):8, (17,2):9, (17,4):10}\n",
    "    return diction[(volume_id, layer_id)]\n",
    "\n",
    "\"\"\"\n",
    "returns dataframe of hits that only lie on cylinders for a given event\n",
    "\"\"\"\n",
    "def get_cylinder_hits(hit_dataframe, truth_dataframe ):\n",
    "    \n",
    "    volume_layer_list = [(8,2),(8,4),(8,6),(8,8),(13,2),(13,4),(13,6),(13,8),(17,2),(17,4)]\n",
    "    temp = [] \n",
    "    merge_by_hit_ids = hit_dataframe.merge(truth_dataframe, how = 'inner', on ='hit_id')\n",
    "    for pair in volume_layer_list:\n",
    "        volume, layer = pair\n",
    "        test_data = merge_by_hit_ids[(merge_by_hit_ids['volume_id'].values == volume) & (merge_by_hit_ids['layer_id'].values == layer)]\n",
    "        temp.append(test_data)\n",
    "    return pd.concat(temp)\n",
    "    \n",
    "#set full_tracks flag if looking for tracks of len 10\n",
    "#set ideal flag if looking for tracks of len 10 and only 1 hit per layer\n",
    "#setting ideal flag will set/override full_tracks flag\n",
    "\n",
    "def gen_simple_tracks(hit_df, truth_df, full_tracks = False, ideal = False, debug=False):\n",
    "    if ideal is True:\n",
    "        full_tracks = True\n",
    "    assert(isinstance(hit_df, pd.DataFrame) and isinstance(truth_df, pd.DataFrame))\n",
    "    hit_and_truth = get_cylinder_hits(hit_df, truth_df)\n",
    "    prelim_tracks = gen_tracks(hit_and_truth).values\n",
    "    if full_tracks is False:\n",
    "        return prelim_tracks \n",
    "    \n",
    "    #full_tracks flag must be true at this point\n",
    "    rows_to_remove = []\n",
    "    for row, track in enumerate(prelim_tracks):\n",
    "        if len(track) is not 10: #10 for each vol-layer in detector\n",
    "            rows_to_remove.append(row)\n",
    "    full_tracks = np.delete(prelim_tracks, rows_to_remove)\n",
    "    if ideal is False:\n",
    "        if debug:\n",
    "            print('prelim track num: '+str(len(prelim_tracks)))\n",
    "            print('removed tracks: '+str(len(rows_to_remove)))\n",
    "            print('remaining tracks: '+str(len(prelim_tracks) - len(rows_to_remove)))\n",
    "        return full_tracks\n",
    "    \n",
    "    #ideal flag must be true at this point\n",
    "    bad_rows = []\n",
    "    compare = [x for x in range(1,11)] #to guarantee each hit in track per layer\n",
    "    for idx, track in enumerate(full_tracks):\n",
    "        layer_nums = []\n",
    "        for hit in track:\n",
    "            vol_id = hit_df.loc[hit_df['hit_id']==hit]['volume_id'].item()\n",
    "            lay_id = hit_df.loc[hit_df['hit_id']==hit]['layer_id'].item()\n",
    "            layer_nums.append(layer_number(vol_id, lay_id))\n",
    "        if layer_nums != compare:\n",
    "            bad_rows.append(idx)\n",
    "    ideal_tracks = np.delete(full_tracks, bad_rows)\n",
    "    if debug:\n",
    "        print('full tracks: '+str(len(full_tracks)))\n",
    "        print('non-ideal tracks: '+str(len(bad_rows)))\n",
    "        print('ideal tracks: '+str(len(ideal_tracks)))\n",
    "    return ideal_tracks \n",
    "            \n",
    "def simple_batch_iter(hit_df, truth_df, batch_size, full_tracks = False, ideal = False):\n",
    "    if ideal is True:\n",
    "        full_tracks = True\n",
    "    tracks = gen_simple_tracks(hit_df, truth_df, full_tracks, ideal)\n",
    "    np.random.shuffle(tracks)\n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "constants needed for seeding\n",
    "\"\"\"\n",
    "#hit_id - 1 is the index of the hit\n",
    "hit_idi = 0\n",
    "ri = 1\n",
    "phii = 2\n",
    "zi = 3\n",
    "volume_idi = 4\n",
    "layer_idi = 5\n",
    "\n",
    "d_phi = 3*np.pi/180\n",
    "d_phi2 = 3*np.pi/180\n",
    "d_z = 5\n",
    "d_r = 5\n",
    "\n",
    "z_max = 20#maximum distance along z axis of track origin\n",
    "\n",
    "r_first_layer = 32.313498434074106\n",
    "r_second_layer = 72.14554366164577\n",
    "r_third_layer = 116.08917912535651\n",
    "z_first_disk_right = 599.9792565549286\n",
    "z_first_disk_left = -z_first_disk_right\n",
    "z_second_disk_right = 699.9354521625164\n",
    "z_second_disk_left = -z_second_disk_right\n",
    "\n",
    "\n",
    "def trans_to_cylindrical (hits):\n",
    "    hits_trans_info = []\n",
    "    for row in hits.itertuples():\n",
    "        hit_id = row.__getattribute__('hit_id')\n",
    "        x = row.__getattribute__('x')\n",
    "        y = row.__getattribute__('y')\n",
    "        z = row.__getattribute__('z')\n",
    "        volume_id = row.__getattribute__('volume_id')\n",
    "        layer_id = row.__getattribute__('layer_id')\n",
    "        r, phi, z = cartesian_to_3d_polar(x,y,z)\n",
    "        hits_trans_info.append([hit_id, r, phi, z, volume_id, layer_id])\n",
    "    hits_trans = pd.DataFrame (hits_trans_info, columns=[\"hit_id\", \"r\", \"phi\", \"z\", \"volume_id\", \"layer_id\"])\n",
    "    return hits_trans\n",
    "\n",
    "\n",
    "def quadrant_shift (phi):\n",
    "    if (phi < 0):\n",
    "        return 2*np.pi + phi\n",
    "    else:\n",
    "        return phi\n",
    "\n",
    "    \n",
    "def extrapolate (x1, x2, x3, y1, y2):\n",
    "    #assert (x2 - x1 != 0)\n",
    "    m = (y2 - y1)/(x2 - x1)\n",
    "    b = y1 - x1*m\n",
    "    y3 = x3*m + b\n",
    "    return y3\n",
    "\n",
    "\n",
    "def create_seeds (hits):#already transformed to cylindrical\n",
    "    \n",
    "    hits_trans = trans_to_cylindrical (hits)\n",
    "    hits_array = np.array (hits_trans.values)\n",
    "    \n",
    "    hits_first_layer = hits_trans.loc[(hits_trans['volume_id'] == 8) & (hits_trans['layer_id'] == 2)]\n",
    "    hits_first_layer_array = hits_first_layer.get_values()\n",
    "    hits_second_layer = hits_trans.loc[(hits_trans['volume_id'] == 8) & (hits_trans['layer_id'] == 4)]\n",
    "    hits_second_layer_array = hits_second_layer.get_values()\n",
    "    hits_third_layer = hits_trans.loc[(hits_trans['volume_id'] == 8) & (hits_trans['layer_id'] == 6)]\n",
    "    hits_third_layer_array = hits_third_layer.get_values()\n",
    "    hits_first_disk_left = hits_trans.loc[(hits_trans['volume_id'] == 7) & (hits_trans['layer_id'] == 14)]\n",
    "    hits_first_disk_left_array = hits_first_disk_left.get_values()\n",
    "    hits_first_disk_right = hits_trans.loc[(hits_trans['volume_id'] == 9) & (hits_trans['layer_id'] == 2)]\n",
    "    hits_first_disk_right_array = hits_first_disk_right.get_values()\n",
    "    hits_second_disk_left = hits_trans.loc[(hits_trans['volume_id'] == 7) & (hits_trans['layer_id'] == 12)]\n",
    "    hits_second_disk_left_array = hits_second_disk_left.get_values()\n",
    "    hits_second_disk_right = hits_trans.loc[(hits_trans['volume_id'] == 9) & (hits_trans['layer_id'] == 4)]\n",
    "    hits_second_disk_right_array = hits_second_disk_right.get_values()\n",
    "    \n",
    "    seeds = []\n",
    "    pool = Pool(32)\n",
    "    parameters = []\n",
    "    for hit in hits_first_layer_array:\n",
    "        parameters.append((hits_array, hits_second_layer_array, hits_third_layer_array, hits_first_disk_left_array, hits_first_disk_right_array, hits_second_disk_left_array, hits_second_disk_right_array, int(np.round(hit[hit_idi]))))\n",
    "    seeds = pool.starmap (create_seeds_from_first_hit, parameters)\n",
    "    #for parameter in parameters:\n",
    "        #create_seeds_from_first_hit (parameter[0], parameter[1], parameter[2], parameter[3], parameter[4], parameter[5], parameter[6], parameter[7])\n",
    "    flattened_seeds = []\n",
    "    for some_seeds in seeds:\n",
    "        for seed in some_seeds:\n",
    "            flattened_seeds.append (seed)\n",
    "    return flattened_seeds\n",
    "\n",
    "\n",
    "def create_seeds_from_first_hit (hits_array, hits_second_layer_array, hits_third_layer_array, hits_first_disk_left_array, hits_first_disk_right_array, hits_second_disk_left_array, hits_second_disk_right_array, hit_id):\n",
    "    seeds = []\n",
    "    hit = hits_array[hit_id - 1]\n",
    "    assert (hit[layer_idi] == 2)\n",
    "    phi = hit[phii]\n",
    "    z = hit[zi]\n",
    "    if (abs(phi) > np.pi/2):\n",
    "        phi = quadrant_shift (phi)\n",
    "    phiMin = phi - d_phi/2\n",
    "    phiMax = phi + d_phi/2\n",
    "    zMin = extrapolate (0, r_first_layer, r_second_layer, z_max, z)\n",
    "    zMax = extrapolate (0, r_first_layer, r_second_layer, -z_max, z)\n",
    "    for thisHit in hits_second_layer_array:\n",
    "        thisPhi = thisHit[phii]\n",
    "        if (phiMax > np.pi):\n",
    "            thisPhi = quadrant_shift (thisPhi)\n",
    "        thisZ = thisHit[zi]\n",
    "        if (thisPhi < phiMax and thisPhi > phiMin and thisZ < zMax and thisZ > zMin):\n",
    "            seeds += create_seeds_from_second_hit (hits_array, hits_third_layer_array, (hit_id, int(np.round(thisHit[hit_idi]))))\n",
    "            #seeds += create_seeds_from_second_hit_first_disk (hits_array, hits_first_disk_left_array, hits_first_disk_right_array, (hit_id, int(np.round(thisHit[hit_idi]))))\n",
    "    return seeds #uncomment above line and remove this line to start doing disks too\n",
    "    r = hit[ri]\n",
    "    if (z > z_max):\n",
    "        rMax = extrapolate (z_max, z, z_first_disk_right, 0, r)\n",
    "        rMin = extrapolate (-z_max, z, z_first_disk_right, 0, r)\n",
    "        for thisHit in hits_first_disk_right_array:\n",
    "            thisPhi = thisHit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = thisHit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                seeds += create_seeds_from_second_hit_second_disk (hits_array, hits_second_disk_left_array, hits_second_disk_right_array, (hit_id, int(np.round(thisHit[hit_idi]))))\n",
    "    if (z < -z_max):\n",
    "        rMin = extrapolate (z_max, z, z_first_disk_left, 0, r)\n",
    "        rMax = extrapolate (-z_max, z, z_first_disk_left, 0, r)\n",
    "        for thisHit in hits_first_disk_left_array:\n",
    "            thisPhi = thisHit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = thisHit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                seeds += create_seeds_from_second_hit_second_disk (hits_array, hits_second_disk_left_array, hits_second_disk_right_array, (hit_id, int(np.round(thisHit[hit_idi]))))\n",
    "    return seeds\n",
    "\n",
    "\n",
    "def create_seeds_from_second_hit (hits_array, hits_third_layer_array, twoSeed):\n",
    "    seeds = []\n",
    "    phi1 = hits_array[twoSeed[0] - 1][phii]\n",
    "    phi2 = hits_array[twoSeed[1] - 1][phii]\n",
    "    r1 = hits_array[twoSeed[0] - 1][ri]\n",
    "    r2 = hits_array[twoSeed[1] - 1][ri]\n",
    "    r3 = r_third_layer\n",
    "    z1 = hits_array[twoSeed[0] - 1][zi]\n",
    "    z2 = hits_array[twoSeed[1] - 1][zi]\n",
    "    phi3 = extrapolate (r1, r2, r3, phi1, phi2)\n",
    "    z3 = extrapolate (r1, r2, r3, z1, z2)\n",
    "    if (abs(phi3) > np.pi/2):\n",
    "        phi3 = quadrant_shift (phi3)\n",
    "    phiMin = phi3 - d_phi2/2\n",
    "    phiMax = phi3 + d_phi2/2\n",
    "    zMin = z3 - d_z/2\n",
    "    zMax = z3 + d_z/2\n",
    "    for hit in hits_third_layer_array:\n",
    "        thisPhi = hit[phii]\n",
    "        if (phiMax > np.pi):\n",
    "            thisPhi = quadrant_shift (thisPhi)\n",
    "        thisZ = hit[zi]\n",
    "        if (thisPhi < phiMax and thisPhi > phiMin and thisZ < zMax and thisZ > zMin):\n",
    "            seeds.append((twoSeed[0], twoSeed[1], int(np.round(hit[0]))))\n",
    "    return seeds\n",
    "\n",
    "\n",
    "def create_seeds_from_second_hit_first_disk (hits_array, hits_first_disk_left_array, hits_first_disk_right_array, twoSeed):\n",
    "    seeds = []\n",
    "    phi1 = hits_array[twoSeed[0] - 1][phii]\n",
    "    phi2 = hits_array[twoSeed[1] - 1][phii]\n",
    "    r1 = hits_array[twoSeed[0] - 1][ri]\n",
    "    r2 = hits_array[twoSeed[1] - 1][ri]\n",
    "    z1 = hits_array[twoSeed[0] - 1][zi]\n",
    "    z2 = hits_array[twoSeed[1] - 1][zi]\n",
    "    if (z2 > z1):\n",
    "        z3 = z_first_disk_right\n",
    "    else:\n",
    "        z3 = z_first_disk_left\n",
    "    phi3 = extrapolate (z1, z2, z3, phi1, phi2)\n",
    "    r3 = extrapolate (z1, z2, z3, r1, r2)\n",
    "    if (abs(phi3) > np.pi/2):\n",
    "        phi3 = quadrant_shift (phi3)\n",
    "    phiMin = phi3 - d_phi2/2\n",
    "    phiMax = phi3 + d_phi2/2\n",
    "    rMin = r3 - d_r/2\n",
    "    rMax = r3 + d_r/2\n",
    "    if (z2 > z1):\n",
    "        for hit in hits_first_disk_right_array:\n",
    "            thisPhi = hit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = hit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                    seeds.append((twoSeed[0], twoSeed[1], int(np.round(hit[0]))))\n",
    "    else:\n",
    "        for hit in hits_first_disk_left_array:\n",
    "            thisPhi = hit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = hit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                    seeds.append((twoSeed[0], twoSeed[1], int(np.round(hit[0]))))\n",
    "    return seeds\n",
    "\n",
    "\n",
    "def create_seeds_from_second_hit_second_disk (hits_array, hits_second_disk_left_array, hits_second_disk_right_array, twoSeed):\n",
    "    seeds = []\n",
    "    phi1 = hits_array[twoSeed[0] - 1][phii]\n",
    "    phi2 = hits_array[twoSeed[1] - 1][phii]\n",
    "    r1 = hits_array[twoSeed[0] - 1][ri]\n",
    "    r2 = hits_array[twoSeed[1] - 1][ri]\n",
    "    z1 = hits_array[twoSeed[0] - 1][zi]\n",
    "    z2 = hits_array[twoSeed[1] - 1][zi]\n",
    "    if (z2 > z1):\n",
    "        z3 = z_second_disk_right\n",
    "    else:\n",
    "        z3 = z_second_disk_left\n",
    "    phi3 = extrapolate (z1, z2, z3, phi1, phi2)\n",
    "    r3 = extrapolate (z1, z2, z3, r1, r2)\n",
    "    if (abs(phi3) > np.pi/2):\n",
    "        phi3 = quadrant_shift (phi3)\n",
    "    phiMin = phi3 - d_phi2/2\n",
    "    phiMax = phi3 + d_phi2/2\n",
    "    rMin = r3 - d_r/2\n",
    "    rMax = r3 + d_r/2\n",
    "    if (z2 > z1):\n",
    "        for hit in hits_second_disk_right_array:\n",
    "            thisPhi = hit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = hit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                    seeds.append((twoSeed[0], twoSeed[1], int(np.round(hit[0]))))\n",
    "    else:\n",
    "        for hit in hits_second_disk_left_array:\n",
    "            thisPhi = hit[phii]\n",
    "            if (phiMax > np.pi):\n",
    "                thisPhi = quadrant_shift (thisPhi)\n",
    "            thisR = hit[ri]\n",
    "            if (thisPhi < phiMax and thisPhi > phiMin and thisR < rMax and thisR > rMin):\n",
    "                    seeds.append((twoSeed[0], twoSeed[1], int(np.round(hit[0]))))\n",
    "    return seeds\n",
    "\n",
    "def rpzlv_from_seeds (seeds, hits):\n",
    "    hits_from_seeds = []\n",
    "    for seed in seeds:\n",
    "        hits_from_seed = []\n",
    "        for hit_id in seed:\n",
    "            hit = hits.loc[hits['hit_id'] == hit_id]\n",
    "            x = hit['x'].values[0]\n",
    "            y = hit['y'].values[0]\n",
    "            z = hit['z'].values[0]\n",
    "            r, phi, z = cartesian_to_3d_polar (x, y, z)\n",
    "            l = hit['layer_id'].values[0]\n",
    "            v = hit['volume_id'].values[0]\n",
    "            hits_from_seed.append ([r, phi, z, l, v])\n",
    "        hits_from_seeds.append (hits_from_seed)\n",
    "    return np.array (hits_from_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
