{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "%matplotlib notebook\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_from_seeds = np.load('./hits_from_seeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_from_seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hits, cells, particles, truth = load_data_single_event(1050)\n",
    "unique_lv_pairs = []\n",
    "for row in hits.iloc[:,4:6].itertuples():\n",
    "    lv_pair = (row[1], row[2])\n",
    "    if lv_pair not in unique_lv_pairs:\n",
    "        unique_lv_pairs.append(lv_pair)\n",
    "\"\"\"\n",
    "fcl_dict = {}\n",
    "for lv_pair in unique_lv_pairs:\n",
    "    lv = str(lv_pair[0]) + \"-\" + str(lv_pair[1])\n",
    "    inputs = tf.placeholder(tf.float32, [None, 1, 3])\n",
    "    num_outputs = 30\n",
    "    layer = tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.relu)\n",
    "    fcl_dict[lv] = layer\n",
    "\"\"\"\n",
    "def get_fcl(layer_id, volume_id):\n",
    "    lv_pair = str(layer_id) + \"-\" + str(volume_id)\n",
    "    for key, value in fcl_dict.items():\n",
    "        if lv_pair == key:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encapsulates the NN used for next-hit prediction \n",
    "\"\"\"\n",
    "class predict_engine(): \n",
    "    N_steps = 10\n",
    "    N_INPUT_FEATURES = 5\n",
    "    N_NEURONS = 200\n",
    "    N_OUTPUTS = 3\n",
    "    MODEL_PATH = './checkpoints/baseline-idealv4/-33103.meta'\n",
    "    def __init__(self):\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        infSess = tf.Session() \n",
    "        init.run(session = infSess)\n",
    "            \n",
    "        saver = tf.train.import_meta_graph(self.MODEL_PATH)\n",
    "        graph = saver.restore(infSess,'./checkpoints/baseline-idealv4/-33103' )\n",
    "        #graph = tf.get_default_graph #hopefully this always points to the right one...\n",
    "        predict_op = infSess.graph.get_tensor_by_name(\"print_op:0\")\n",
    "        self.predict_op = predict_op\n",
    "        self.infSess = infSess \n",
    "    \n",
    "    def pred(self):\n",
    "        x_data, y_data, _, __ = next(next_batch(self.N_steps, 1, self.N_INPUT_FEATURES, ideal = True))\n",
    "        \"\"\"\n",
    "            Need to modify pred function to not use train set data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        inputs = tf.placeholder(tf.float32, [None, 10,5 ])\n",
    "        input_tensor = self.infSess.graph.get_tensor_by_name('input_ph:0')\n",
    "        prediction = self.infSess.run(self.predict_op, feed_dict={input_tensor:x_data})\n",
    "        print('input was: ')\n",
    "        print(x_data)\n",
    "        print('\\n')\n",
    "        print('pred is: ')\n",
    "        print(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def close(self):\n",
    "        self.infSess.close()\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = predict_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_hit(hits_so_far):\n",
    "    n_steps = len(hits_so_far[0])\n",
    "    n_input_features = 5\n",
    "    n_neurons = 200\n",
    "    n_outputs = 3\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input')\n",
    "    \n",
    "    lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "    lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs, reuse=tf.AUTO_REUSE)\n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    print_op = tf.Print(rnn_outputs, [rnn_outputs], name='print_pred')\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as lstm3sess:\n",
    "\n",
    "        init.run()\n",
    "        initial_step = global_step.eval()\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter('./logs/lstm3', lstm3sess.graph)\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/lstm3/'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(lstm3sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        x_data = hits_so_far\n",
    "        test_print = lstm3sess.run(print_op, feed_dict={X:x_data})\n",
    "        return [test_print[0][-1][0], test_print[0][-1][1], test_print[0][-1][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1050)\n",
    "num_seeds = len(hits_from_seeds)\n",
    "seeded_hits = next_seed(hits_from_seeds)\n",
    "parameters = []\n",
    "parameters = [[10, 100, num_seeds]] * len(hits_from_seeds)\n",
    "for i in range(len(hits_from_seeds)):\n",
    "    parameters[i].append([10, 100, num_seeds, next(seeded_hits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pool = Pool(4)\n",
    "tracks, mse = pool.starmap(predict_tracks, parameters)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tracks(max_hits_per_track, error, num_seeds):\n",
    "    #grab hit_id per hit\n",
    "    seeded_hits = next_seed(hits_from_seeds)\n",
    "    hits_from_tracks = []\n",
    "    mse_list = []\n",
    "    for i in range(num_seeds):\n",
    "        feed_seed = next(seeded_hits)\n",
    "        count = 0\n",
    "        mse_list.append([])\n",
    "        while (len(feed_seed[0]) <= max_hits_per_tracks - 1):\n",
    "            count = count + 1\n",
    "            predicted_hit = predict_next_hit(feed_seed)\n",
    "            next_hit = get_xyz (closestHit (predicted_hit[0], predicted_hit[1], predicted_hit[2], hits) )\n",
    "            hit_info = get_hit_info(next_hit[0], next_hit[1], next_hit[2])\n",
    "            next_hit.extend((hit_info[0], hit_info[1]))\n",
    "            mse = distance_between_two_points(predicted_hit, next_hit)\n",
    "            mse_list[i].append(mse)\n",
    "            if mse > error:\n",
    "                break\n",
    "            feed_seed[0].append(next_hit)\n",
    "        hits_from_tracks.append(feed_seed)\n",
    "    return hits_from_tracks, mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1050)\n",
    "#do MSE (distance between predicted and real) for every hit per track\n",
    "#grab hit_id per hit\n",
    "error = 100\n",
    "max_hits_per_tracks = 10\n",
    "seeded_hits = next_seed(hits_from_seeds)\n",
    "hits_from_tracks = []\n",
    "mse_list = []\n",
    "start = time.time()\n",
    "for i in range(len(hits_from_seeds)):\n",
    "    print(\"iteration: \" + str(i))\n",
    "    feed_seed = next(seeded_hits)\n",
    "    count = 0\n",
    "    mse_list.append([])\n",
    "    while (len(feed_seed[0]) <= max_hits_per_tracks - 1):\n",
    "        count = count + 1\n",
    "        #print(\"num predicted hits: \" + str(count))\n",
    "        #print(\"hits so far (feed_seed): \", feed_seed[0])\n",
    "        predicted_hit = predict_next_hit(feed_seed)\n",
    "        #print(\"predicted_hit: \", predicted_hit)\n",
    "        next_hit = get_xyz (closestHit (predicted_hit[0], predicted_hit[1], predicted_hit[2], hits) )\n",
    "        hit_info = get_hit_info(next_hit[0], next_hit[1], next_hit[2])\n",
    "        next_hit.extend((hit_info[0], hit_info[1]))\n",
    "        #print(\"next hit in sequence: \", next_hit)\n",
    "        mse = distance_between_two_points(predicted_hit, next_hit)\n",
    "        mse_list[i].append(mse)\n",
    "        if mse > error:\n",
    "            break\n",
    "        feed_seed[0].append(next_hit)\n",
    "        #print(feed_seed[0])\n",
    "    #print(feed_seed)\n",
    "    hits_from_tracks.append(feed_seed)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
