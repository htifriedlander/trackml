{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "%run utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hits: tensor of samples x timesteps x features\n",
    "    bounds: K:V of x_max, x_min, y_max, y_min, z_max, z_min\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "assuming input is samples x [x_val,y_val,z_val]\n",
    "\"\"\"\n",
    "def calc_norm(xyz, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    norm_x = ((xyz[0] - x_min)/(x_max - x_min))\n",
    "    norm_y = ((xyz[1] - y_min)/(y_max - y_min))\n",
    "    norm_z = ((xyz[2] - z_min)/(z_max - z_min))\n",
    "    return [norm_x,norm_y,norm_z]\n",
    "\n",
    "def norm_hits(hits, bounds):\n",
    "    assert ('x_max' and 'x_min' and 'y_max' and 'y_min' and 'z_max' and 'z_min' in bounds) or ('r_max' and 'r_min' and 'phi_max' and 'phi_min' and 'z_max' and 'z_min' in bounds)\n",
    "    return np.apply_along_axis(calc_norm, 1, hits, x_min = bounds['x_min'], x_max = bounds['x_max'], y_min = bounds['y_min'], y_max = bounds['y_max'], z_min = bounds['z_min'], z_max = bounds['z_max']) \n",
    "\n",
    "\n",
    "def calc_bounds():\n",
    "    bounds={'x_max':0, 'x_min':0, 'y_max':0, 'y_min':0, 'z_max':0, 'z_min':0}\n",
    "    for num in range(1000,1099):\n",
    "        file_name = 'event00000' + str(num)\n",
    "        hits, cells, particles, truth = load_event('data/train_sample/'+file_name)\n",
    "        event_x_max, event_x_min, event_y_max, event_y_min, event_z_max, event_z_min = hits['x'].max(), hits['x'].min(), hits['y'].max(), hits['y'].min(), hits['z'].max(), hits['z'].min() \n",
    "        if bounds['x_max'] < event_x_max:\n",
    "            bounds['x_max'] = event_x_max\n",
    "        if bounds['x_min'] > event_x_min:\n",
    "            bounds['x_min'] = event_x_min\n",
    "        if bounds['y_max'] < event_y_max:\n",
    "            bounds['y_max'] = event_y_max\n",
    "        if bounds['y_min'] > event_y_min:\n",
    "            bounds['y_min'] = event_y_min\n",
    "        if bounds['z_max'] < event_z_max:\n",
    "            bounds['z_max'] = event_z_max\n",
    "        if bounds['z_min'] > event_y_min:\n",
    "            bounds['z_min'] = event_y_min\n",
    "    return bounds\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "input: hits and truth dfs\n",
    "\n",
    "output: df of particle_id as index and list of particles \n",
    "\"\"\"\n",
    "\n",
    "def gen_tracks(truth_df):\n",
    "    assert(isinstance(truth_df, pd.DataFrame))\n",
    "    #print(truth_df)\n",
    "    truth_df['dist'] = np.sqrt(truth_df['tx']**2+truth_df['ty']**2+truth_df['tz']**2)\n",
    "    grouped = truth_df.groupby('particle_id')['hit_id','dist']\n",
    "    a = grouped.apply(lambda x: x.sort_values(by=['dist'],ascending=True))\n",
    "    final = a.groupby('particle_id')['hit_id'].apply(np.array)\n",
    "    return final \n",
    "\n",
    "##there's still a problem with how to deal with hits that have a particle id of 0\n",
    "\n",
    "def batch_iter(truth_df, batch_size):\n",
    "    tracks = gen_tracks(truth_df).values\n",
    "    np.random.shuffle(tracks) \n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "        \n",
    "def get_data(max_seq_len, batch_size, feature_len, truth_df, hits_df):\n",
    "    hits = hits_df\n",
    "    max_seq_len = max_seq_len\n",
    "    b_size = batch_size\n",
    "    features = feature_len #xyz or phi r z\n",
    "    all_data = list(batch_iter(truth_df,b_size))\n",
    "    \n",
    "    #print(all_data)\n",
    "    for result in all_data:\n",
    "        batch = []\n",
    "        labels_tensor = []\n",
    "        for track_list in result:\n",
    "            for hit_id in track_list:\n",
    "                hit_coord = []\n",
    "                track = []\n",
    "                for elem in hit_id:\n",
    "                    x,y,z = hits.loc[hits['hit_id']== elem]['x'].item(), hits.loc[hits['hit_id']== elem]['y'].item(), hits.loc[hits['hit_id']== elem]['z'].item()\n",
    "                    r,phi,z = cartesian_to_3d_polar(x,y,z)\n",
    "                    hit_coord = [r,phi,z]\n",
    "                    track.append(hit_coord)\n",
    "                zeros_to_add = max_seq_len - len(track)\n",
    "                if zeros_to_add > 0:\n",
    "                    add_array = np.zeros((zeros_to_add,3))\n",
    "                    np_data = np.array(track) \n",
    "                    padded_track_data  = np.append(np_data,add_array,axis=0)\n",
    "                elif zeros_to_add < 0:\n",
    "                    modded_track = track[:zeros_to_add]\n",
    "                    padded_track_data = np.array(modded_track)\n",
    "                else:\n",
    "                    padded_track_data = np.array(track)\n",
    "            row_label = padded_track_data[1:]\n",
    "            padded_row_label = np.append(row_label, np.zeros((1,3)),axis=0)\n",
    "            labels_tensor.append(padded_row_label)\n",
    "            batch.append(padded_track_data)\n",
    "        padded_batch_data = np.array(batch)\n",
    "        padded_labels = np.array(labels_tensor)\n",
    "        #print(padded_labels)\n",
    "        yield padded_batch_data, padded_labels\n",
    "        \n",
    "def next_batch(max_seq_len, batch_size, feature_len):\n",
    "    all_data = load_dataset('data/train_sample/', parts=['hits','truth'])\n",
    "    for data in all_data:\n",
    "        hit_df, truth_df = data[1], data[2]\n",
    "        yield from get_data(max_seq_len, batch_size, feature_len, truth_df, hit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_steps = 7\n",
    "n_input_features = 3\n",
    "n_neurons = 200\n",
    "n_outputs = 3\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input_features] )\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#uses tanh for inner activations\n",
    "lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype = tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('./checkpoints-61300.meta')\n",
    "saver.restore (sess, tf.train.latest_checkpoint ('./'))\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_iterations = 100000\n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    init.run()\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        x_data, y_data = next(next_batch(n_steps, (n_iterations*100), n_input_features))\n",
    "        _, loss_batch, summary = sess.run([train_op, batch_loss, summary_op], feed_dict={x:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=iteration)\n",
    "        \n",
    "        if iteration % 100 ==0:\n",
    "            mse = batch_loss.eval(feed_dict={X:x_batch, y:y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "            saver.save(sess, './checkpoints', iteration)\n",
    "    #saver.save(sess,\"./lstm_weights1.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints'))\n",
    "\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello?\n",
      "INFO:tensorflow:Restoring parameters from checkpoints-0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 3, 3) for Tensor 'input:0', which has shape '(?, 7, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b8d71530e814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(x_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(y_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mshow_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfSess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_to_restore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input is:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1116\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1117\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 3, 3) for Tensor 'input:0', which has shape '(?, 7, 3)'"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_steps = 7\n",
    "n_input_features = 3\n",
    "n_neurons = 200\n",
    "n_outputs = 3\n",
    "restoreGraph = tf.train.import_meta_graph('checkpoints-0.meta')\n",
    "with tf.Session() as infSess:\n",
    "    print('hello?')\n",
    "    restoreGraph.restore(infSess, 'checkpoints-0')\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    #print('here?')\n",
    "    inputs = graph.get_tensor_by_name(\"input:0\")\n",
    "    #label_op = graph.get_tensor_by_name(\"label\")\n",
    "    #print('how about now?')\n",
    "    opt_to_restore = graph.get_tensor_by_name(\"print_pred:0\")\n",
    "    for i in range(100):\n",
    "        #print('helloooooooooo?')\n",
    "        x_data, y_data = next(next_batch(n_steps, 1, n_input_features))\n",
    "        #print(x_data)\n",
    "        #print(y_data)\n",
    "        show_pred = infSess.run(opt_to_restore, feed_dict={inputs:x_data})\n",
    "        print('input is:')\n",
    "        print(x_data)\n",
    "        print('model pred is:')\n",
    "        print(show_pred)\n",
    "        print('true track is:')\n",
    "        print(y_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls checkpoints-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
