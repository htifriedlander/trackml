{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32.31738261840044, 72.15091393575189, 116.0946312221527, 172.07253929769357, 260.3569293324722, 360.3101542051449, 500.2151479081191, 660.12508634391, 820.1546870692688, 1020.1106909393304]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "%matplotlib notebook\n",
    "%run utils.ipynb\n",
    "%run all_create.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1050)\n",
    "hits_from_seeds = np.load('./hits_from_seeds.npy')\n",
    "print (hits.head())\n",
    "hits_array = np.array (hits.get_values())\n",
    "mapping = np.argsort (hits_array[:,0])\n",
    "hits_array = hits_array[mapping]\n",
    "print (hits_array)\n",
    "hits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef predict_next_hit(hits_so_far):\\n    n_steps = len(hits_so_far[0])\\n    n_input_features = 5\\n    n_neurons = 200\\n    n_outputs = 3\\n\\n    X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input')\\n    \\n    lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\\n    lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs, reuse=tf.AUTO_REUSE)\\n    rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\\n    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\\n    print_op = tf.Print(rnn_outputs, [rnn_outputs], name='print_pred')\\n    init = tf.global_variables_initializer()\\n    \\n    with tf.Session() as lstm3sess:\\n\\n        init.run()\\n        initial_step = global_step.eval()\\n        saver = tf.train.Saver()\\n        writer = tf.summary.FileWriter('./logs/lstm3', lstm3sess.graph)\\n        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/lstm3/'))\\n        # if that checkpoint exists, restore from checkpoint\\n        if ckpt and ckpt.model_checkpoint_path:\\n            saver.restore(lstm3sess, ckpt.model_checkpoint_path)\\n\\n        x_data = hits_so_far\\n        test_print = lstm3sess.run(print_op, feed_dict={X:x_data})\\n        return [test_print[0][-1][0], test_print[0][-1][1], test_print[0][-1][2]]\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def predict_next_hit(hits_so_far):\n",
    "    n_steps = len(hits_so_far[0])\n",
    "    n_input_features = 5\n",
    "    n_neurons = 200\n",
    "    n_outputs = 3\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input')\n",
    "    \n",
    "    lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "    lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs, reuse=tf.AUTO_REUSE)\n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    print_op = tf.Print(rnn_outputs, [rnn_outputs], name='print_pred')\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as lstm3sess:\n",
    "\n",
    "        init.run()\n",
    "        initial_step = global_step.eval()\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter('./logs/lstm3', lstm3sess.graph)\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/lstm3/'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(lstm3sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        x_data = hits_so_far\n",
    "        test_print = lstm3sess.run(print_op, feed_dict={X:x_data})\n",
    "        return [test_print[0][-1][0], test_print[0][-1][1], test_print[0][-1][2]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Encapsulates the NN used for next-hit prediction \n",
    "\"\"\"\n",
    "class predict_engine(): \n",
    "    N_steps = 10\n",
    "    N_INPUT_FEATURES = 5\n",
    "    N_NEURONS = 200\n",
    "    N_OUTPUTS = 3\n",
    "    MODEL_PATH = './checkpoints/baseline-idealv4/-33103.meta'\n",
    "    \n",
    "    def __init__(self):\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        infSess = tf.Session() \n",
    "        init.run(session = infSess)\n",
    "            \n",
    "        saver = tf.train.import_meta_graph(self.MODEL_PATH)\n",
    "        graph = saver.restore(infSess,'./checkpoints/baseline-idealv4/-33103' )\n",
    "        #graph = tf.get_default_graph #hopefully this always points to the right one...\n",
    "        predict_op = infSess.graph.get_tensor_by_name(\"print_op:0\")\n",
    "        self.predict_op = predict_op\n",
    "        self.infSess = infSess \n",
    "    \n",
    "    def pred(self, seed):\n",
    "        #print('seed shape is:')\n",
    "        #print(seed.shape)\n",
    "        pad_zeros = 10 - len(seed)\n",
    "        x_data = np.array([np.pad(seed, ((0, pad_zeros), (0,0)), 'constant')])\n",
    "        #print('x_data shape is:')\n",
    "        #print(x_data.shape)\n",
    "        \n",
    "        inputs = tf.placeholder(tf.float32, [None, 10, 5])\n",
    "        input_tensor = self.infSess.graph.get_tensor_by_name('input_ph:0')\n",
    "        prediction = self.infSess.run(self.predict_op, feed_dict={input_tensor:x_data})\n",
    "        #print('input was: ')\n",
    "        #print(x_data)\n",
    "        #print('\\n')\n",
    "        #print('pred is: ')\n",
    "        #print(prediction)\n",
    "        return prediction[0]\n",
    "    \n",
    "    def close(self):\n",
    "        self.infSess.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#tf_init_start = time.time()\n",
    "prediction = predict_engine()\n",
    "prediction.load()\n",
    "#tf_init_end = time.time()\n",
    "\n",
    "#init_seed_start = time.time()\n",
    "seeds = next_seed(hits_from_seeds)\n",
    "#init_seed_end = time.time()\n",
    "\n",
    "\n",
    "hits_from_tracks = []\n",
    "mse_hitid = []\n",
    "#austin_start_time = time.time()\n",
    "\n",
    "#total_pred_time = 0\n",
    "#total_track_pass_time = 0\n",
    "#get_xyz_total = 0\n",
    "#get_hit_info_total = 0\n",
    "#mse_total = 0\n",
    "#process_time_total = 0\n",
    "for i in range(5):#change it back to 5\n",
    "    mse_hitid.append([])\n",
    "    feed_seed = next(seeds)[0]\n",
    "    #print('feed_seed is: ')\n",
    "    #print(feed_seed)\n",
    "    \n",
    "    for j in range(7):\n",
    "        #print(\"j: \", j)\n",
    "       \n",
    "        #pred_start = time.time()\n",
    "        predicted_hit = prediction.pred(feed_seed)[j+2]\n",
    "        #pred_end = time.time()\n",
    "        \n",
    "        \n",
    "        #print('NN pred time: ' + str(pred_end -pred_start))\n",
    "        \n",
    "        predicted_hit_xyz = polar_to_cartesian(predicted_hit[0], predicted_hit[1], predicted_hit[2])\n",
    "        \n",
    "                \n",
    "        #track_pass_init = time.time()\n",
    "        real_hit_id = calc_dist(predicted_hit_xyz[0], predicted_hit_xyz[1], predicted_hit_xyz[2], hits_array)\n",
    "        #track_pass_end = time.time()\n",
    "        \n",
    "        \n",
    "        #get_xyz_init = time.time()\n",
    "        real_hit = get_xyz(real_hit_id)\n",
    "        #get_xyz_end = time.time()\n",
    "        #get_xyz_total +=(get_xyz_end - get_xyz_init)\n",
    "        \n",
    "        #get_hit_info_init = time.time()\n",
    "        l, v, hit_id = get_hit_info(real_hit[0], real_hit[1], real_hit[2]) #change function to take hits df as parameter\n",
    "        #get_hit_info_end = time.time()\n",
    "        #get_hit_info_total += (get_hit_info_end - get_hit_info_init)\n",
    "        \n",
    "        #mse_init = time.time()\n",
    "        mse = distance_between_two_points(real_hit, polar_to_cartesian(predicted_hit[0], predicted_hit[1], predicted_hit[2]))\n",
    "        #mse_end = time.time()\n",
    "        #reshape_init = time.time()\n",
    "        mse_hitid[i].extend([[mse, real_hit_id]])\n",
    "        real_hit.extend([l,v])\n",
    "        reshaped_real_hit = np.asarray(real_hit).reshape(1, 5)\n",
    "        feed_seed = np.append(feed_seed, reshaped_real_hit, axis=0)\n",
    "        #reshape_end = time.time()\n",
    "        \n",
    "        \n",
    "        #mse_total += (mse_end - mse_init)\n",
    "        #process_time_total += (reshape_end - reshape_init)\n",
    "        #total_pred_time += (pred_end - pred_start)\n",
    "        #total_track_pass_time +=(track_pass_end - track_pass_init)\n",
    "        #print('track pass time: '+str(track_pass_end - track_pass_init))\n",
    "    #print ('this time it was ' + str (pred_end - pred_start))\n",
    "    #print ('but track pass time was ' + str (track_pass_end - track_pass_init))\n",
    "    hits_from_tracks.append(feed_seed)\n",
    "#final_end = time.time()\n",
    "#print('total runtime: ' + str(final_end -austin_start_time))\n",
    "#print('get xyz_total time: ',get_xyz_total)\n",
    "#print('get hit info total: ',get_hit_info_total)\n",
    "#print('tf graph load: ' + str(tf_init_end - tf_init_start))\n",
    "#print('total NN pred time: ',total_pred_time)\n",
    "#print('total track pass time: ',total_track_pass_time)\n",
    "#print('total mse calc time: ', mse_total)\n",
    "#print('reshaping time: ', process_time_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_track_from_seed (feed_seed, hits_array):\n",
    "    mse_hitid = []\n",
    "    for j in range(7):\n",
    "        predicted_hit = prediction.pred(feed_seed)[j+2]\n",
    "        predicted_hit_xyz = polar_to_cartesian(predicted_hit[0], predicted_hit[1], predicted_hit[2])\n",
    "        (real_hit_id, dist_squared) = calc_dist(predicted_hit_xyz[0], predicted_hit_xyz[1], predicted_hit_xyz[2], hits_array)\n",
    "        hit_index = int (np.round (real_hit_id - 1))\n",
    "        assert (hits_array[hit_index,0] == real_hit_id)\n",
    "        x, y, z, l, v = hits_array[hit_index,1], hits_array[hit_index,2], hits_array[hit_index,3], hits_array[hit_index,5], hits_array[hit_index,4]\n",
    "        r, phi, z = cartesian_to_3d_polar (x, y, z)\n",
    "        to_append = np.array ([[r, phi, z, l, v]])\n",
    "        feed_seed = np.append (feed_seed, to_append, axis = 0)\n",
    "        mse_hitid.append ([dist_squared, real_hit_id])\n",
    "        continue\n",
    "    return (feed_seed, mse_hitid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_event (seeds, hits):\n",
    "    hits_array = np.array (hits.get_values())\n",
    "    hits_from_seeds = rpzlv_from_seeds (seeds, hits)\n",
    "    seeds_generated = next_seed(hits_from_seeds)\n",
    "    hits_from_tracks = []\n",
    "    mse_hitid = []\n",
    "    count = 0\n",
    "    for feed_seed in seeds_generated:\n",
    "        feed_seed = np.array (feed_seed[0])\n",
    "        seed_mse_hitid = [[0, seeds[count,0]],[0, seeds[count,1]],[0, seeds[count,2]]]\n",
    "        (feed_seed, track_mse_hitid) = reconstruct_track_from_seed (feed_seed, hits_array)\n",
    "        hits_from_tracks.append (feed_seed)\n",
    "        mse_hitid.append (seed_mse_hitid + track_mse_hitid)\n",
    "        count += 1\n",
    "       \n",
    "    return (hits_from_tracks, mse_hitid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_test_events ():\n",
    "    itr = load_dataset ('./data/test_data', parts = ['hits'])\n",
    "    for event_data in itr:\n",
    "        event_id = event_data[0]\n",
    "        print ('now working on event number ' + str (event_id))\n",
    "        if (event_id <= 80):\n",
    "            continue\n",
    "        hits = event_data[1]\n",
    "        hits_trans = trans_to_cylindrical (hits)\n",
    "        seeds = np.load ('./seeds/test_event_' + str(event_id) + '_.npy')\n",
    "        yield reconstruct_event (seeds, hits), event_id#yield (hits_from_event, hit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/allv1/model.ckpt\n",
      "now working on event number 0\n",
      "now working on event number 1\n",
      "now working on event number 2\n",
      "now working on event number 3\n",
      "now working on event number 4\n",
      "now working on event number 5\n",
      "now working on event number 6\n",
      "now working on event number 7\n",
      "now working on event number 8\n",
      "now working on event number 9\n",
      "now working on event number 10\n",
      "now working on event number 11\n",
      "now working on event number 12\n",
      "now working on event number 13\n",
      "now working on event number 14\n",
      "now working on event number 15\n",
      "now working on event number 16\n",
      "now working on event number 17\n",
      "now working on event number 18\n",
      "now working on event number 19\n",
      "now working on event number 20\n",
      "now working on event number 21\n",
      "now working on event number 22\n",
      "now working on event number 23\n",
      "now working on event number 24\n",
      "now working on event number 25\n",
      "now working on event number 26\n",
      "now working on event number 27\n",
      "now working on event number 28\n",
      "now working on event number 29\n",
      "now working on event number 30\n",
      "now working on event number 31\n",
      "now working on event number 32\n",
      "now working on event number 33\n",
      "now working on event number 34\n",
      "now working on event number 35\n",
      "now working on event number 36\n",
      "now working on event number 37\n",
      "now working on event number 38\n",
      "now working on event number 39\n",
      "now working on event number 40\n",
      "now working on event number 41\n",
      "now working on event number 42\n",
      "now working on event number 43\n",
      "now working on event number 44\n",
      "now saving event number 44\n",
      "elapsed time is 0.17059538708792793 hours.\n",
      "now working on event number 45\n",
      "now saving event number 45\n",
      "elapsed time is 0.24281095465024313 hours.\n",
      "now working on event number 46\n",
      "now saving event number 46\n",
      "elapsed time is 0.3020277843872706 hours.\n",
      "now working on event number 47\n",
      "now saving event number 47\n",
      "elapsed time is 0.34912833392620085 hours.\n",
      "now working on event number 48\n",
      "now saving event number 48\n",
      "elapsed time is 0.4241667864057753 hours.\n",
      "now working on event number 49\n",
      "now saving event number 49\n",
      "elapsed time is 0.5328210020727581 hours.\n",
      "now working on event number 50\n",
      "now saving event number 50\n",
      "elapsed time is 0.6484334694014655 hours.\n",
      "now working on event number 51\n",
      "now saving event number 51\n",
      "elapsed time is 0.7494837036397722 hours.\n",
      "now working on event number 52\n",
      "now saving event number 52\n",
      "elapsed time is 0.8516287875175477 hours.\n",
      "now working on event number 53\n",
      "now saving event number 53\n",
      "elapsed time is 0.9483937442964978 hours.\n",
      "now working on event number 54\n",
      "now saving event number 54\n",
      "elapsed time is 1.0553596489959294 hours.\n",
      "now working on event number 55\n",
      "now saving event number 55\n",
      "elapsed time is 1.1141497796111637 hours.\n",
      "now working on event number 56\n",
      "now saving event number 56\n",
      "elapsed time is 1.2160193132691914 hours.\n",
      "now working on event number 57\n",
      "now saving event number 57\n",
      "elapsed time is 1.3421148424016105 hours.\n",
      "now working on event number 58\n",
      "now saving event number 58\n",
      "elapsed time is 1.4651955637666914 hours.\n",
      "now working on event number 59\n",
      "now saving event number 59\n",
      "elapsed time is 1.5592168823215695 hours.\n",
      "now working on event number 60\n",
      "now saving event number 60\n",
      "elapsed time is 1.6326106599304413 hours.\n",
      "now working on event number 61\n",
      "now saving event number 61\n",
      "elapsed time is 1.715034006966485 hours.\n",
      "now working on event number 62\n",
      "now saving event number 62\n",
      "elapsed time is 1.816016249126858 hours.\n",
      "now working on event number 63\n",
      "now saving event number 63\n",
      "elapsed time is 1.9306225016382006 hours.\n",
      "now working on event number 64\n",
      "now saving event number 64\n",
      "elapsed time is 2.037671831978692 hours.\n",
      "now working on event number 65\n",
      "now saving event number 65\n",
      "elapsed time is 2.096875924534268 hours.\n",
      "now working on event number 66\n",
      "now saving event number 66\n",
      "elapsed time is 2.183988422486517 hours.\n",
      "now working on event number 67\n",
      "now saving event number 67\n",
      "elapsed time is 2.320714568429523 hours.\n",
      "now working on event number 68\n",
      "now saving event number 68\n",
      "elapsed time is 2.405409941209687 hours.\n",
      "now working on event number 69\n",
      "now saving event number 69\n",
      "elapsed time is 2.4732748331626255 hours.\n",
      "now working on event number 70\n",
      "now saving event number 70\n",
      "elapsed time is 2.5445294472244053 hours.\n",
      "now working on event number 71\n",
      "now saving event number 71\n",
      "elapsed time is 2.6578625714116626 hours.\n",
      "now working on event number 72\n",
      "now saving event number 72\n",
      "elapsed time is 2.743405215607749 hours.\n",
      "now working on event number 73\n",
      "now saving event number 73\n",
      "elapsed time is 2.8351880927218334 hours.\n",
      "now working on event number 74\n",
      "now saving event number 74\n",
      "elapsed time is 2.9424172460370595 hours.\n",
      "now working on event number 75\n",
      "now saving event number 75\n",
      "elapsed time is 3.0162391667895845 hours.\n",
      "now working on event number 76\n",
      "now saving event number 76\n",
      "elapsed time is 3.0908577024274404 hours.\n",
      "now working on event number 77\n",
      "now saving event number 77\n",
      "elapsed time is 3.139196069041888 hours.\n",
      "now working on event number 78\n",
      "now saving event number 78\n",
      "elapsed time is 3.227625808583366 hours.\n",
      "now working on event number 79\n",
      "now saving event number 79\n",
      "elapsed time is 3.2790700333648255 hours.\n",
      "now working on event number 80\n",
      "now saving event number 80\n",
      "elapsed time is 3.418780163526535 hours.\n",
      "now working on event number 81\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b13b83276a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhits_from_tracks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_hitid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreconstruct_test_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'now saving event number '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'elapsed time is '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' hours.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f583f73cd6d1>\u001b[0m in \u001b[0;36mreconstruct_test_events\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mhits_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_to_cylindrical\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'./seeds/test_event_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mreconstruct_event\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_id\u001b[0m\u001b[0;31m#yield (hits_from_event, hit_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8fb9c9f8538f>\u001b[0m in \u001b[0;36mreconstruct_event\u001b[0;34m(seeds, hits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct_event\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mhits_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhits_from_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpzlv_from_seeds\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mseeds_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits_from_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhits_from_tracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bd703a255130>\u001b[0m in \u001b[0;36mrpzlv_from_seeds\u001b[0;34m(seeds, hits)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mhits_from_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhit_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mhit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hit_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhit_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "prediction = predict_engine()\n",
    "prediction.load()\n",
    "start_time = time.time()\n",
    "for (hits_from_tracks, mse_hitid), event_id in reconstruct_test_events():\n",
    "    print ('now saving event number ' + str (event_id))\n",
    "    print ('elapsed time is ' + str ((time.time() - start_time)/60/60) + ' hours.')\n",
    "    np.save ('./reconstructed_test_events/hits_from_tracks_event_' + str (event_id) + '.npy', np.array (hits_from_tracks))\n",
    "    np.save ('./reconstructed_test_events/mse_hitid_event_' + str (event_id) + '.npy', np.array (mse_hitid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "prediction = predict_engine()\n",
    "prediction.load()\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "number_to_do = len (hits_from_seeds)\n",
    "\n",
    "for feed_seed in seeds:#change it back to 5\n",
    "    feed_seed = np.array (feed_seed[0])\n",
    "    (feed_seed, track_mse_hitid) = reconstruct_track_from_seed (feed_seed)\n",
    "    hits_from_tracks.append (feed_seed)\n",
    "    mse_hitid.append (track_mse_hitid)\n",
    "    #print (counter % int (number_to_do/100))\n",
    "    if (counter % int (number_to_do/100) k== 0):\n",
    "        print (counter/number_to_do)\n",
    "    counter += 1\n",
    "\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "prediction = predict_engine()\n",
    "prediction.load()\n",
    "hits, cells, particles, truth = load_data_single_event(1050)\n",
    "seeds = np.load ('./seeds/training_event_1050.npy')\n",
    "(hits_from_tracks, mse_hitid) = reconstruct_event (seeds, hits)\n",
    "np.save ('./hits_from_tracks.npy', np.array (hits_from_tracks))\n",
    "np.save ('./mse_hitid.npy', np.array (mse_hitid))\n",
    "print('pipeline time for single event:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save ('./tracksToPlot.npy', np.array (hits_from_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = np.load('./mse_hitid.npy')\n",
    "track_cand = np.load('./hits_from_tracks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_cand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
