{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # To-Do:\n",
    "1. Construct baseline model + saving specification \n",
    "2. Calculate epoch specifications \n",
    "3. Figure out how to tie graph to GPU device \n",
    "4. start baseline training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "import os\n",
    "import math \n",
    "import time\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 10\n",
    "N_EPOCHS = 3\n",
    "INPUTS_DIM = 5\n",
    "OUTPUTS_DIM = 3\n",
    "LSTM_NEURONS = 400\n",
    "LEARN_RATE = 0.0005 \n",
    "BATCH_SIZE = 50\n",
    "n_iterations = 1000000000000000000000\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/baseline-idealv4/-19302\n",
      "0 \tMSE: 54.158375\n",
      "100 \tMSE: 57.281895\n",
      "200 \tMSE: 94.15044\n",
      "300 \tMSE: 97.87722\n",
      "400 \tMSE: 77.16811\n",
      "500 \tMSE: 48.507458\n",
      "600 \tMSE: 75.20193\n",
      "700 \tMSE: 33.33127\n",
      "800 \tMSE: 29.551706\n",
      "900 \tMSE: 48.722046\n",
      "1000 \tMSE: 38.667408\n",
      "1100 \tMSE: 57.85515\n",
      "1200 \tMSE: 51.395897\n",
      "1300 \tMSE: 30.696016\n",
      "1400 \tMSE: 510.6043\n",
      "1500 \tMSE: 39.591095\n",
      "1600 \tMSE: 32.449646\n",
      "1700 \tMSE: 41.39968\n",
      "1800 \tMSE: 36.389782\n",
      "1900 \tMSE: 40.102757\n",
      "2000 \tMSE: 34.95057\n",
      "2100 \tMSE: 41.551907\n",
      "2200 \tMSE: 30.315434\n",
      "2300 \tMSE: 25.112068\n",
      "2400 \tMSE: 52.183426\n",
      "2500 \tMSE: 32.96162\n",
      "2600 \tMSE: 20.011097\n",
      "2700 \tMSE: 48.377827\n",
      "2800 \tMSE: 75.404594\n",
      "2900 \tMSE: 71.95022\n",
      "3000 \tMSE: 29.8967\n",
      "3100 \tMSE: 31.342691\n",
      "3200 \tMSE: 23.355536\n",
      "3300 \tMSE: 38.792133\n",
      "3400 \tMSE: 48.060482\n",
      "3500 \tMSE: 60.873055\n",
      "3600 \tMSE: 25.671043\n",
      "3700 \tMSE: 47.16125\n",
      "3800 \tMSE: 84.69738\n",
      "3900 \tMSE: 132.8261\n",
      "4000 \tMSE: 33.807964\n",
      "4100 \tMSE: 26.006746\n",
      "4200 \tMSE: 22.934845\n",
      "4300 \tMSE: 34.3207\n",
      "4400 \tMSE: 35.551353\n",
      "4500 \tMSE: 212.44955\n",
      "4600 \tMSE: 56.06098\n",
      "4700 \tMSE: 50.460716\n",
      "4800 \tMSE: 124.015335\n",
      "4900 \tMSE: 40.654255\n",
      "5000 \tMSE: 48.1861\n",
      "5100 \tMSE: 25.604246\n",
      "5200 \tMSE: 34.69732\n",
      "5300 \tMSE: 22.711164\n",
      "5400 \tMSE: 33.974\n",
      "5500 \tMSE: 21.399187\n",
      "5600 \tMSE: 31.27067\n",
      "5700 \tMSE: 68.614365\n",
      "5800 \tMSE: 56.240753\n",
      "5900 \tMSE: 43.28255\n",
      "6000 \tMSE: 31.684706\n",
      "6100 \tMSE: 32.889614\n",
      "6200 \tMSE: 27.638893\n",
      "6300 \tMSE: 31.424484\n",
      "6400 \tMSE: 28.314913\n",
      "6500 \tMSE: 34.634697\n",
      "6600 \tMSE: 26.9804\n",
      "6700 \tMSE: 28.82631\n",
      "6800 \tMSE: 42.305153\n",
      "6900 \tMSE: 32.786663\n",
      "7000 \tMSE: 27.39178\n",
      "7100 \tMSE: 47.677597\n",
      "7200 \tMSE: 26.117706\n",
      "7300 \tMSE: 383.84808\n",
      "7400 \tMSE: 39.636116\n",
      "7500 \tMSE: 32.86908\n",
      "7600 \tMSE: 39.65176\n",
      "7700 \tMSE: 53.692665\n",
      "7800 \tMSE: 208.684\n",
      "7900 \tMSE: 37.76669\n",
      "8000 \tMSE: 395.14795\n",
      "8100 \tMSE: 28.840242\n",
      "8200 \tMSE: 31.110825\n",
      "8300 \tMSE: 47.81147\n",
      "8400 \tMSE: 49.861027\n",
      "8500 \tMSE: 33.855667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name = 'global_step')\n",
    "#with tf.device('/device:GPU:0'):\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, N_STEPS, INPUTS_DIM], name = 'input_ph' )\n",
    "Y = tf.placeholder(tf.float32, [None, N_STEPS, OUTPUTS_DIM], name = 'label_ph' )\n",
    "#constructing model\n",
    "\n",
    "lstm = tf.contrib.rnn.LSTMCell(num_units = LSTM_NEURONS, use_peepholes = True)\n",
    "#adding FC layer\n",
    "lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = OUTPUTS_DIM)\n",
    "rnn_outputs, hidden_states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "\n",
    "#prediction = tf.Variable(rnn_outputs([BATCH_SIZE, N_STEPS, OUTPUTS_DIM]), name = 'prediction')\n",
    "#print_op might not be necessary\n",
    "\n",
    "loss_diff = tf.reduce_sum(tf.square(tf.subtract(rnn_outputs, Y)), 2)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(Y), 2))\n",
    "mask_loss = mask*loss_diff\n",
    "batch_loss = tf.reduce_mean(mask_loss)\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=LEARN_RATE)\n",
    "train_op = train.minimize(batch_loss, global_step=global_step)\n",
    "\n",
    "print_op = tf.Print(rnn_outputs, [rnn_outputs], name = 'print_op')\n",
    "init=tf.global_variables_initializer()\n",
    "    #data_itr = next_batch(N_STEPS, BATCH_SIZE, INPUTS_DIM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', batch_loss)\n",
    "    tf.summary.histogram('histogram_loss', batch_loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as baseline_idealv4:\n",
    "    init.run()\n",
    "    initial_step = global_step.eval()\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./logs/baseline-idealv4/', baseline_idealv4.graph)\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/baseline-idealv4/'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(baseline_idealv4, ckpt.model_checkpoint_path)\n",
    "    \n",
    "\n",
    "    data_itr = next_batch(N_STEPS, BATCH_SIZE, INPUTS_DIM, simple = True, full_tracks = True, ideal = True, full_data = True)\n",
    "    epochs = 0\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):\n",
    "        \n",
    "        try:\n",
    "            x_batch, y_batch, _, __ = next(data_itr)\n",
    "\n",
    "            ___, loss_batch, summary, test_print = baseline_idealv4.run([train_op, batch_loss, summary_op, print_op], feed_dict = {X:x_batch, Y:y_batch})\n",
    "        \n",
    "            writer.add_summary(summary, global_step = global_step.eval(session=baseline_idealv4))\n",
    "            #print('input:')\n",
    "            #print(x_batch)\n",
    "            #print('pred:')\n",
    "            #print(test_print)\n",
    "            if iteration %100 ==0:\n",
    "                mse = batch_loss.eval(feed_dict={X:x_batch, Y:y_batch})\n",
    "                print(iteration, \"\\tMSE:\", mse)\n",
    "                saver.save(baseline_idealv4, './checkpoints/baseline-idealv4/', global_step = global_step)\n",
    "        except StopIteration as e:\n",
    "            epochs +=1\n",
    "            if epocs == n_epochs:\n",
    "                print('finished all epochs')\n",
    "                baseline_idealv4.close()\n",
    "                break\n",
    "            else:\n",
    "                data_itr = next_batch(N_STEPS, BATCH_SIZE, INPUTS_DIM, simple = True, full_tracks = True, ideal = True, full_data = True)\n",
    "                end = time.time()\n",
    "                print('elasped time for 1st epoch: '+str(end-start))\n",
    "                pass\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
