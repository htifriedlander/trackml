{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_from_seeds = np.load('./hits_from_seeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_data_single_event(1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hits: tensor of samples x timesteps x features\n",
    "    bounds: K:V of x_max, x_min, y_max, y_min, z_max, z_min\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "assuming input is samples x [x_val,y_val,z_val]\n",
    "\"\"\"\n",
    "def calc_norm(xyz, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    norm_x = ((xyz[0] - x_min)/(x_max - x_min))\n",
    "    norm_y = ((xyz[1] - y_min)/(y_max - y_min))\n",
    "    norm_z = ((xyz[2] - z_min)/(z_max - z_min))\n",
    "    return [norm_x,norm_y,norm_z]\n",
    "\n",
    "def norm_hits(hits, bounds):\n",
    "    assert ('x_max' and 'x_min' and 'y_max' and 'y_min' and 'z_max' and 'z_min' in bounds) or ('r_max' and 'r_min' and 'phi_max' and 'phi_min' and 'z_max' and 'z_min' in bounds)\n",
    "    return np.apply_along_axis(calc_norm, 1, hits, x_min = bounds['x_min'], x_max = bounds['x_max'], y_min = bounds['y_min'], y_max = bounds['y_max'], z_min = bounds['z_min'], z_max = bounds['z_max']) \n",
    "\n",
    "\n",
    "def calc_bounds():\n",
    "    bounds={'x_max':0, 'x_min':0, 'y_max':0, 'y_min':0, 'z_max':0, 'z_min':0}\n",
    "    for num in range(1000,1099):\n",
    "        file_name = 'event00000' + str(num)\n",
    "        hits, cells, particles, truth = load_event('data/train_sample/'+file_name)\n",
    "        event_x_max, event_x_min, event_y_max, event_y_min, event_z_max, event_z_min = hits['x'].max(), hits['x'].min(), hits['y'].max(), hits['y'].min(), hits['z'].max(), hits['z'].min() \n",
    "        if bounds['x_max'] < event_x_max:\n",
    "            bounds['x_max'] = event_x_max\n",
    "        if bounds['x_min'] > event_x_min:\n",
    "            bounds['x_min'] = event_x_min\n",
    "        if bounds['y_max'] < event_y_max:\n",
    "            bounds['y_max'] = event_y_max\n",
    "        if bounds['y_min'] > event_y_min:\n",
    "            bounds['y_min'] = event_y_min\n",
    "        if bounds['z_max'] < event_z_max:\n",
    "            bounds['z_max'] = event_z_max\n",
    "        if bounds['z_min'] > event_y_min:\n",
    "            bounds['z_min'] = event_y_min\n",
    "    return bounds\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "input: hits and truth dfs\n",
    "\n",
    "output: df of particle_id as index and list of particles \n",
    "\"\"\"\n",
    "\n",
    "def gen_tracks(truth_df):\n",
    "    assert(isinstance(truth_df, pd.DataFrame))\n",
    "    #print(truth_df)\n",
    "    truth_df['dist'] = np.sqrt(truth_df['tx']**2+truth_df['ty']**2+truth_df['tz']**2)\n",
    "    grouped = truth_df.groupby('particle_id')['hit_id','dist']\n",
    "    a = grouped.apply(lambda x: x.sort_values(by=['dist'],ascending=True))\n",
    "    final = a.groupby('particle_id')['hit_id'].apply(np.array)\n",
    "    return final \n",
    "\n",
    "##there's still a problem with how to deal with hits that have a particle id of 0\n",
    "\n",
    "def batch_iter(truth_df, batch_size):\n",
    "    tracks = gen_tracks(truth_df).values\n",
    "    np.random.shuffle(tracks) \n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "        \n",
    "def get_data(max_seq_len, batch_size, feature_len, truth_df, hits_df):\n",
    "    hits = hits_df\n",
    "    max_seq_len = max_seq_len\n",
    "    b_size = batch_size\n",
    "    features = feature_len #xyz or phi r z\n",
    "    all_data = list(batch_iter(truth_df,b_size))\n",
    "    \n",
    "    #print(all_data)\n",
    "    for result in all_data:\n",
    "        batch = []\n",
    "        batch_lv = []\n",
    "        labels_tensor = []\n",
    "        labels_tensor_lv = []\n",
    "        for track_list in result:\n",
    "            for hit_id in track_list:\n",
    "                hit_coord = []\n",
    "                track = []\n",
    "                track_lb = []\n",
    "                lv_pair = []\n",
    "                lv_tensor = []\n",
    "                label_coord = []\n",
    "                for elem in hit_id:\n",
    "                    x, y, z, layer_id, volume_id = hits.loc[hits['hit_id']== elem]['x'].item(), hits.loc[hits['hit_id']== elem]['y'].item(), hits.loc[hits['hit_id']== elem]['z'].item(), hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    r,phi,z = cartesian_to_3d_polar(x,y,z)\n",
    "                    hit_coord = [r,phi,z,layer_id, volume_id]\n",
    "                    label_coord = [r,phi,z]\n",
    "                    track.append(hit_coord)\n",
    "                    track_lb.append(label_coord)\n",
    "                    layer, volume = hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    lv_pair = [layer, volume]\n",
    "                    lv_tensor.append(lv_pair)\n",
    "                zeros_to_add = max_seq_len - len(track)\n",
    "                if zeros_to_add > 0:\n",
    "                    add_array = np.zeros((zeros_to_add,feature_len))\n",
    "                    add_array_lb = np.zeros((zeros_to_add, 3)) #3 is hardcoded for xyz/rphiz\n",
    "                    add_array_lv = np.zeros((zeros_to_add, 2))\n",
    "                    np_data = np.array(track)\n",
    "                    np_data_lb = np.array(track_lb)\n",
    "                    np_data_lv = np.array(lv_tensor)\n",
    "                    padded_track_data  = np.append(np_data,add_array,axis=0)\n",
    "                \n",
    "                    padded_track_data_lb = np.append(np_data_lb, add_array_lb, axis=0)\n",
    "                    padded_track_data_lv = np.append(np_data_lv, add_array_lv, axis=0)\n",
    "                elif zeros_to_add < 0:\n",
    "                    modded_track = track[:zeros_to_add]\n",
    "                    modded_track_lv = lv_tensor[:zeros_to_add]\n",
    "                    modded_track_lb = track_lb[:zeros_to_add]\n",
    "                    padded_track_data_lb = np.array(modded_track_lb)\n",
    "                    padded_track_data = np.array(modded_track)\n",
    "                    padded_track_data_lv = np.array(modded_track_lv)\n",
    "                else:\n",
    "                    padded_track_data_lb = np.array(track_lb)\n",
    "                    padded_track_data = np.array(track)\n",
    "                    padded_track_data_lv = np.array(lv_tensor)\n",
    "            \n",
    "            row_label = padded_track_data_lb[1:]\n",
    "            row_label_lv = padded_track_data_lv[1:]\n",
    "            padded_row_label = np.append(row_label, np.zeros((1,3)), axis=0) #hardcoded 3 for xyz/rphiz\n",
    "            padded_row_label_lv = np.append(row_label_lv, np.zeros((1,2)), axis=0)\n",
    "            labels_tensor.append(padded_row_label)\n",
    "            labels_tensor_lv.append(padded_row_label_lv)\n",
    "            batch.append(padded_track_data)\n",
    "            batch_lv.append(padded_track_data_lv)\n",
    "        padded_batch_data = np.array(batch)\n",
    "        padded_batch_data_lv = np.array(batch_lv)\n",
    "        padded_labels = np.array(labels_tensor)\n",
    "        padded_labels_lv = np.array(labels_tensor_lv)\n",
    "        #print(padded_labels)\n",
    "        #print(padded_labels_lv)\n",
    "        yield padded_batch_data, padded_labels, padded_batch_data_lv, padded_labels_lv\n",
    "        \n",
    "def next_batch(max_seq_len, batch_size, feature_len):\n",
    "    all_data = load_dataset('data/train_sample/', parts=['hits','truth'])\n",
    "    for data in all_data:\n",
    "        hit_df, truth_df = data[1], data[2]\n",
    "        yield from get_data(max_seq_len, batch_size, feature_len, truth_df, hit_df)\n",
    "        \n",
    "def get_lv_id(x, y, z):\n",
    "    row = hits.loc[(hits['x'] == x) & (hits['y'] == y) & (hits['z'] == z)]\n",
    "    return int(row.iloc[0][4]), int(row.iloc[0][5])\n",
    "\n",
    "def next_seed(hits_from_seeds):\n",
    "    for seed in hits_from_seeds:\n",
    "        track_seed = seed.reshape(1,3,5)\n",
    "        yield track_seed.tolist()\n",
    "        \n",
    "def distance_between_two_points(p1, p2):\n",
    "    distance = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2 + (p2[2] - p1[2])**2)\n",
    "    return distance\n",
    "\n",
    "def get_xyz(hit_id):\n",
    "    xyz = hits[hits[\"hit_id\"] == hit_id].iloc[:,1:4].values.tolist()[0]\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lv_pairs = []\n",
    "for row in hits.iloc[:,4:6].itertuples():\n",
    "    lv_pair = (row[1], row[2])\n",
    "    if lv_pair not in unique_lv_pairs:\n",
    "        unique_lv_pairs.append(lv_pair)\n",
    "\n",
    "fcl_dict = {}\n",
    "for lv_pair in unique_lv_pairs:\n",
    "    lv = str(lv_pair[0]) + \"-\" + str(lv_pair[1])\n",
    "    inputs = tf.placeholder(tf.float32, [None, 1, 3])\n",
    "    num_outputs = 30\n",
    "    layer = tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.relu)\n",
    "    fcl_dict[lv] = layer\n",
    "\n",
    "def get_fcl(layer_id, volume_id):\n",
    "    lv_pair = str(layer_id) + \"-\" + str(volume_id)\n",
    "    for key, value in fcl_dict.items():\n",
    "        if lv_pair == key:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_hit(hits_so_far):\n",
    "    n_steps = len(hits_so_far[0])\n",
    "    n_input_features = 5\n",
    "    n_neurons = 200\n",
    "    n_outputs = 3\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input')\n",
    "    \n",
    "    lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "    lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs, reuse=tf.AUTO_REUSE)\n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    print_op = tf.Print(rnn_outputs, [rnn_outputs], name='print_pred')\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as lstm3sess:\n",
    "\n",
    "        init.run()\n",
    "        initial_step = global_step.eval()\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter('./logs/lstm3', lstm3sess.graph)\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/lstm3/'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(lstm3sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        x_data = hits_so_far\n",
    "        test_print = lstm3sess.run(print_op, feed_dict={X:x_data})\n",
    "        return [test_print[0][-1][0], test_print[0][-1][1], test_print[0][-1][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 100\n",
    "max_hits_per_tracks = 10\n",
    "seeded_hits = next_seed(hits_from_seeds)\n",
    "hits_from_tracks = []\n",
    "for i in range(2):\n",
    "    print(\"iteration: \" + str(i))\n",
    "    feed_seed = next(seeded_hits)\n",
    "    count = 0\n",
    "    while (len(feed_seed[0]) <= max_hits_per_tracks - 1):\n",
    "        count = count + 1\n",
    "        print(\"num predicted hits: \" + str(count))\n",
    "        print(\"hits so far (feed_seed): \", feed_seed[0])\n",
    "        predicted_hit = predict_next_hit(feed_seed)\n",
    "        print(\"predicted_hit: \", predicted_hit)\n",
    "        next_hit = get_xyz (closestHit (predicted_hit[0], predicted_hit[1], predicted_hit[2], hits) )\n",
    "        lv = get_lv_id(next_hit[0], next_hit[1], next_hit[2])\n",
    "        next_hit.extend((lv[0], lv[1]))\n",
    "        print(\"next hit in sequence: \", next_hit)\n",
    "        if distance_between_two_points(predicted_hit, next_hit) > error:\n",
    "            break\n",
    "        feed_seed[0].append(next_hit)\n",
    "        print(feed_seed[0])\n",
    "    print(feed_seed)\n",
    "    hits_from_tracks.append(feed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28380, 3, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits_from_seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
