{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trackml.dataset import load_event, load_dataset \n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event \n",
    "import os\n",
    "import math\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hits, cells, particles, truth = load_data_single_event(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hits: tensor of samples x timesteps x features\n",
    "    bounds: K:V of x_max, x_min, y_max, y_min, z_max, z_min\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "assuming input is samples x [x_val,y_val,z_val]\n",
    "\"\"\"\n",
    "def calc_norm(xyz, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    norm_x = ((xyz[0] - x_min)/(x_max - x_min))\n",
    "    norm_y = ((xyz[1] - y_min)/(y_max - y_min))\n",
    "    norm_z = ((xyz[2] - z_min)/(z_max - z_min))\n",
    "    return [norm_x,norm_y,norm_z]\n",
    "\n",
    "def norm_hits(hits, bounds):\n",
    "    assert ('x_max' and 'x_min' and 'y_max' and 'y_min' and 'z_max' and 'z_min' in bounds) or ('r_max' and 'r_min' and 'phi_max' and 'phi_min' and 'z_max' and 'z_min' in bounds)\n",
    "    return np.apply_along_axis(calc_norm, 1, hits, x_min = bounds['x_min'], x_max = bounds['x_max'], y_min = bounds['y_min'], y_max = bounds['y_max'], z_min = bounds['z_min'], z_max = bounds['z_max']) \n",
    "\n",
    "\n",
    "def calc_bounds():\n",
    "    bounds={'x_max':0, 'x_min':0, 'y_max':0, 'y_min':0, 'z_max':0, 'z_min':0}\n",
    "    for num in range(1000,1099):\n",
    "        file_name = 'event00000' + str(num)\n",
    "        hits, cells, particles, truth = load_event('data/train_sample/'+file_name)\n",
    "        event_x_max, event_x_min, event_y_max, event_y_min, event_z_max, event_z_min = hits['x'].max(), hits['x'].min(), hits['y'].max(), hits['y'].min(), hits['z'].max(), hits['z'].min() \n",
    "        if bounds['x_max'] < event_x_max:\n",
    "            bounds['x_max'] = event_x_max\n",
    "        if bounds['x_min'] > event_x_min:\n",
    "            bounds['x_min'] = event_x_min\n",
    "        if bounds['y_max'] < event_y_max:\n",
    "            bounds['y_max'] = event_y_max\n",
    "        if bounds['y_min'] > event_y_min:\n",
    "            bounds['y_min'] = event_y_min\n",
    "        if bounds['z_max'] < event_z_max:\n",
    "            bounds['z_max'] = event_z_max\n",
    "        if bounds['z_min'] > event_y_min:\n",
    "            bounds['z_min'] = event_y_min\n",
    "    return bounds\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "input: hits and truth dfs\n",
    "\n",
    "output: df of particle_id as index and list of particles \n",
    "\"\"\"\n",
    "\n",
    "def gen_tracks(truth_df):\n",
    "    assert(isinstance(truth_df, pd.DataFrame))\n",
    "    #print(truth_df)\n",
    "    truth_df['dist'] = np.sqrt(truth_df['tx']**2+truth_df['ty']**2+truth_df['tz']**2)\n",
    "    grouped = truth_df.groupby('particle_id')['hit_id','dist']\n",
    "    a = grouped.apply(lambda x: x.sort_values(by=['dist'],ascending=True))\n",
    "    final = a.groupby('particle_id')['hit_id'].apply(np.array)\n",
    "    return final \n",
    "\n",
    "##there's still a problem with how to deal with hits that have a particle id of 0\n",
    "\n",
    "def batch_iter(truth_df, batch_size):\n",
    "    tracks = gen_tracks(truth_df).values\n",
    "    np.random.shuffle(tracks) \n",
    "    remainder = len(tracks) % batch_size if len(tracks) % batch_size is not 0 else 0\n",
    "    if remainder is not 0:\n",
    "        modded_tracks = tracks[:-remainder]\n",
    "    else:\n",
    "        modded_tracks = tracks \n",
    "    assert(len(modded_tracks)%batch_size is 0)\n",
    "    for batch in modded_tracks.reshape(-1,batch_size,1):\n",
    "        yield batch\n",
    "        \n",
    "def get_data(max_seq_len, batch_size, feature_len, truth_df, hits_df):\n",
    "    hits = hits_df\n",
    "    max_seq_len = max_seq_len\n",
    "    b_size = batch_size\n",
    "    features = feature_len #xyz or phi r z\n",
    "    all_data = list(batch_iter(truth_df,b_size))\n",
    "    \n",
    "    #print(all_data)\n",
    "    for result in all_data:\n",
    "        batch = []\n",
    "        batch_lv = []\n",
    "        labels_tensor = []\n",
    "        labels_tensor_lv = []\n",
    "        for track_list in result:\n",
    "            for hit_id in track_list:\n",
    "                hit_coord = []\n",
    "                track = []\n",
    "                track_lb = []\n",
    "                lv_pair = []\n",
    "                lv_tensor = []\n",
    "                label_coord = []\n",
    "                for elem in hit_id:\n",
    "                    x, y, z, layer_id, volume_id = hits.loc[hits['hit_id']== elem]['x'].item(), hits.loc[hits['hit_id']== elem]['y'].item(), hits.loc[hits['hit_id']== elem]['z'].item(), hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    r,phi,z = cartesian_to_3d_polar(x,y,z)\n",
    "                    hit_coord = [r,phi,z,layer_id, volume_id]\n",
    "                    label_coord = [r,phi,z]\n",
    "                    track.append(hit_coord)\n",
    "                    track_lb.append(label_coord)\n",
    "                    layer, volume = hits.loc[hits['hit_id'] == elem]['volume_id'].item(), hits.loc[hits['hit_id'] == elem]['layer_id'].item()\n",
    "                    lv_pair = [layer, volume]\n",
    "                    lv_tensor.append(lv_pair)\n",
    "                zeros_to_add = max_seq_len - len(track)\n",
    "                if zeros_to_add > 0:\n",
    "                    add_array = np.zeros((zeros_to_add,feature_len))\n",
    "                    add_array_lb = np.zeros((zeros_to_add, 3)) #3 is hardcoded for xyz/rphiz\n",
    "                    add_array_lv = np.zeros((zeros_to_add, 2))\n",
    "                    np_data = np.array(track)\n",
    "                    np_data_lb = np.array(track_lb)\n",
    "                    np_data_lv = np.array(lv_tensor)\n",
    "                    padded_track_data  = np.append(np_data,add_array,axis=0)\n",
    "                \n",
    "                    padded_track_data_lb = np.append(np_data_lb, add_array_lb, axis=0)\n",
    "                    padded_track_data_lv = np.append(np_data_lv, add_array_lv, axis=0)\n",
    "                elif zeros_to_add < 0:\n",
    "                    modded_track = track[:zeros_to_add]\n",
    "                    modded_track_lv = lv_tensor[:zeros_to_add]\n",
    "                    modded_track_lb = track_lb[:zeros_to_add]\n",
    "                    padded_track_data_lb = np.array(modded_track_lb)\n",
    "                    padded_track_data = np.array(modded_track)\n",
    "                    padded_track_data_lv = np.array(modded_track_lv)\n",
    "                else:\n",
    "                    padded_track_data_lb = np.array(track_lb)\n",
    "                    padded_track_data = np.array(track)\n",
    "                    padded_track_data_lv = np.array(lv_tensor)\n",
    "            \n",
    "            row_label = padded_track_data_lb[1:]\n",
    "            row_label_lv = padded_track_data_lv[1:]\n",
    "            padded_row_label = np.append(row_label, np.zeros((1,3)), axis=0) #hardcoded 3 for xyz/rphiz\n",
    "            padded_row_label_lv = np.append(row_label_lv, np.zeros((1,2)), axis=0)\n",
    "            labels_tensor.append(padded_row_label)\n",
    "            labels_tensor_lv.append(padded_row_label_lv)\n",
    "            batch.append(padded_track_data)\n",
    "            batch_lv.append(padded_track_data_lv)\n",
    "            \n",
    "        padded_batch_data = np.array(batch)\n",
    "        padded_batch_data_lv = np.array(batch_lv)\n",
    "        padded_labels = np.array(labels_tensor)\n",
    "        padded_labels_lv = np.array(labels_tensor_lv)\n",
    "        #print(padded_labels)\n",
    "        #print(padded_labels_lv)\n",
    "        yield padded_batch_data, padded_labels, padded_batch_data_lv, padded_labels_lv\n",
    "        \n",
    "def next_batch(max_seq_len, batch_size, feature_len):\n",
    "    all_data = load_dataset('data/train_sample/', parts=['hits','truth'])\n",
    "    for data in all_data:\n",
    "        hit_df, truth_df = data[1], data[2]\n",
    "        yield from get_data(max_seq_len, batch_size, feature_len, truth_df, hit_df)\n",
    "        \n",
    "def get_lv_id(x, y, z):\n",
    "    row = hits.loc[(hits['x'] == x) & (hits['y'] == y) & (hits['z'] == z)]\n",
    "    return (int(row.iloc[0][4]), int(row.iloc[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = next_batch(19,1,5)\n",
    "x, labels, _, __ = next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make a new tensor of the same dimension as x, but instead of x,y,z, each point is volume/layer ID\n",
    "#add lookup code in generator\n",
    "#FCL for each volume/layer ID pair\n",
    "#input: 3D vector\n",
    "#output: \n",
    "\n",
    "unique_lv_pairs = []\n",
    "for row in hits.iloc[:,4:6].itertuples():\n",
    "    lv_pair = (row[1], row[2])\n",
    "    if lv_pair not in unique_lv_pairs:\n",
    "        unique_lv_pairs.append(lv_pair)\n",
    "\n",
    "fcl_dict = {}\n",
    "for lv_pair in unique_lv_pairs:\n",
    "    lv = str(lv_pair[0]) + \"-\" + str(lv_pair[1])\n",
    "    inputs = tf.placeholder(tf.float32, [None, 1, 3])\n",
    "    num_outputs = 30\n",
    "    layer = tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.relu)\n",
    "    fcl_dict[lv] = layer\n",
    "\n",
    "def get_fcl(layer_id, volume_id):\n",
    "    lv_pair = str(layer_id) + \"-\" + str(volume_id)\n",
    "    for key, value in fcl_dict.items():\n",
    "        if lv_pair == key:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 7\n",
    "n_input_features = 5\n",
    "n_neurons = 200\n",
    "n_outputs = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_input_features], name='input' )\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs], name='label')\n",
    "    \n",
    "#uses tanh for inner activations\n",
    "lstm = tf.contrib.rnn.LSTMCell(num_units = n_neurons, use_peepholes = True)\n",
    "#lstm = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units = n_neurons)\n",
    "\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(lstm, output_size = n_outputs)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype = tf.float32)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "print_op = tf.Print(rnn_outputs,[rnn_outputs], name='print_pred')\n",
    "loss_diff = tf.reduce_sum(tf.square(rnn_outputs-y), 2)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(y),2))\n",
    "mask_loss = mask*loss_diff\n",
    "batch_loss = tf.reduce_mean(mask_loss)\n",
    "\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = opt.minimize(batch_loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "data_itr = next_batch(n_steps, batch_size,n_input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 340311.94\n",
      "100 \tMSE: 353767.94\n",
      "200 \tMSE: 237066.17\n",
      "300 \tMSE: 372085.53\n",
      "400 \tMSE: 432045.3\n",
      "500 \tMSE: 315972.4\n",
      "600 \tMSE: 314309.72\n",
      "700 \tMSE: 333183.5\n",
      "800 \tMSE: 266048.5\n",
      "900 \tMSE: 250634.48\n",
      "1000 \tMSE: 120459.21\n",
      "1100 \tMSE: 188162.92\n",
      "1200 \tMSE: 251840.72\n",
      "1300 \tMSE: 248650.66\n",
      "1400 \tMSE: 150028.42\n",
      "1500 \tMSE: 151188.52\n",
      "1600 \tMSE: 189437.48\n",
      "1700 \tMSE: 193739.72\n",
      "1800 \tMSE: 261068.45\n",
      "1900 \tMSE: 75797.96\n",
      "2000 \tMSE: 128856.26\n",
      "2100 \tMSE: 157265.61\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 10000\n",
    "batch_size = 50\n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', batch_loss)\n",
    "        tf.summary.histogram('histogram_loss', batch_loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "with tf.Session() as lstm2sess:\n",
    "   \n",
    "    #saver.restore(sess, \"./lstm_weight1sckpt\")\n",
    "    init.run()\n",
    "    initial_step = global_step.eval()\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./logs/lstm2', lstm2sess.graph)\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/lstm2/'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(lstm2sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        x_batch, y_batch, _, __ = next(data_itr)\n",
    "        #print(x_batch.shape)\n",
    "        #print(y_batch.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ___, loss_batch, summary, test_print = lstm2sess.run([train_op, batch_loss, summary_op, print_op], feed_dict={X:x_batch, y:y_batch})\n",
    "        writer.add_summary(summary, global_step=iteration)\n",
    "        \n",
    "        if iteration %100 ==0:\n",
    "            mse = batch_loss.eval(feed_dict={X:x_batch, y:y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "            saver.save(lstm2sess, './checkpoints/lstm2', iteration)\n",
    "    #saver.save(sess,\"./lstm_weights1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
